{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b889b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow\n",
    "import time\n",
    "import cv2\n",
    "import mss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D,Flatten,Input\n",
    "from keras.optimizers import *\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "import ctypes\n",
    "from ctypes import wintypes, c_int, byref\n",
    "import time, random\n",
    "\n",
    "user32 = ctypes.WinDLL('user32', use_last_error=True)\n",
    "\n",
    "INPUT_MOUSE    = 0\n",
    "INPUT_KEYBOARD = 1\n",
    "INPUT_HARDWARE = 2\n",
    "\n",
    "KEYEVENTF_EXTENDEDKEY = 0x0001\n",
    "KEYEVENTF_KEYUP       = 0x0002\n",
    "KEYEVENTF_UNICODE     = 0x0004\n",
    "KEYEVENTF_SCANCODE    = 0x0008\n",
    "\n",
    "MAPVK_VK_TO_VSC = 0\n",
    "\n",
    "# msdn.microsoft.com/en-us/library/dd375731\n",
    "VK_LMENU    = 0x12 # Left Alt key 18\n",
    "VK_LCONTROL = 0xA2 # Left Ctrl key 162\n",
    "\n",
    "# Playstation valid buttons\n",
    "DPAD_UP     = 0x57 # W key 87\n",
    "DPAD_DOWN   = 0x53 # S key 83\n",
    "DPAD_LEFT   = 0x41 # A key 65\n",
    "DPAD_RIGHT  = 0x44 # D key 68\n",
    "CROSS       = 0x45 # E key 69\n",
    "SQUARE      = 0x52 # R key 82\n",
    "CIRCLE      = 0x54 # T key 84\n",
    "TRIANGLE    = 0x59 # Y key 89\n",
    "R1          = 0x51 # Q key 81\n",
    "TOUCHPAD    = 0x58 # X l1 key 88\n",
    "\n",
    "# Diagonal directional buttons with arbitrary values\n",
    "DIAG_DOWN_LEFT = (DPAD_LEFT, DPAD_DOWN)\n",
    "DIAG_DOWN_RIGHT = (DPAD_RIGHT, DPAD_DOWN)\n",
    "DIAG_UP_LEFT = (DPAD_LEFT, DPAD_UP)\n",
    "DIAG_UP_RIGHT = (DPAD_RIGHT, DPAD_UP)\n",
    "\n",
    "# Multi button attacks with arbitrary values\n",
    "CROSS_SQUARE = (0,(CROSS, SQUARE))\n",
    "CROSS_CIRCLE = (TRIANGLE,TRIANGLE,TRIANGLE,TRIANGLE)\n",
    "SQUARE_TRIANGLE = (CROSS,TRIANGLE)\n",
    "TRIANGLE_CIRCLE = (0,(TRIANGLE, CIRCLE))\n",
    "\n",
    "\n",
    "ATTACK01 = (SQUARE,TRIANGLE,DPAD_RIGHT,DPAD_RIGHT,TRIANGLE,DPAD_RIGHT,CIRCLE)\n",
    "\n",
    "\n",
    "delay = ['hold','tap']\n",
    "# available inputs by type\n",
    "direction = [0, DPAD_UP, DPAD_DOWN, DPAD_LEFT, DPAD_RIGHT, DIAG_DOWN_LEFT, DIAG_DOWN_RIGHT, DIAG_UP_LEFT, DIAG_UP_RIGHT]\n",
    "attack = [ATTACK01,TRIANGLE, CIRCLE, CROSS, SQUARE,0, DPAD_UP, DPAD_DOWN, DPAD_LEFT, DPAD_RIGHT, DIAG_DOWN_LEFT, DIAG_DOWN_RIGHT, DIAG_UP_LEFT, DIAG_UP_RIGHT,\n",
    "           CROSS_SQUARE, CROSS_CIRCLE, SQUARE_TRIANGLE, TRIANGLE_CIRCLE,R1,]\n",
    "rage = [R1]\n",
    "\n",
    "# all valid actions\n",
    "valid_actions = attack\n",
    "# valid_actions = [(x,y) for x in direction for y in attack]\n",
    "\n",
    "wintypes.ULONG_PTR = wintypes.WPARAM\n",
    "\n",
    "class MOUSEINPUT(ctypes.Structure):\n",
    "    _fields_ = ((\"dx\",          wintypes.LONG),\n",
    "                (\"dy\",          wintypes.LONG),\n",
    "                (\"mouseData\",   wintypes.DWORD),\n",
    "                (\"dwFlags\",     wintypes.DWORD),\n",
    "                (\"time\",        wintypes.DWORD),\n",
    "                (\"dwExtraInfo\", wintypes.ULONG_PTR))\n",
    "\n",
    "class KEYBDINPUT(ctypes.Structure):\n",
    "    _fields_ = ((\"wVk\",         wintypes.WORD),\n",
    "                (\"wScan\",       wintypes.WORD),\n",
    "                (\"dwFlags\",     wintypes.DWORD),\n",
    "                (\"time\",        wintypes.DWORD),\n",
    "                (\"dwExtraInfo\", wintypes.ULONG_PTR))\n",
    "\n",
    "    \n",
    "\n",
    "    def __init__(self, *args, **kwds):\n",
    "        super(KEYBDINPUT, self).__init__(*args, **kwds)\n",
    "        # some programs use the scan code even if KEYEVENTF_SCANCODE\n",
    "        # isn't set in dwFflags, so attempt to map the correct code.\n",
    "        if not self.dwFlags & KEYEVENTF_UNICODE:\n",
    "            self.wScan = user32.MapVirtualKeyExW(self.wVk,\n",
    "                                                 MAPVK_VK_TO_VSC, 0)\n",
    "class HARDWAREINPUT(ctypes.Structure):\n",
    "    _fields_ = ((\"uMsg\",    wintypes.DWORD),\n",
    "                (\"wParamL\", wintypes.WORD),\n",
    "                (\"wParamH\", wintypes.WORD))\n",
    "\n",
    "class INPUT(ctypes.Structure):\n",
    "    class _INPUT(ctypes.Union):\n",
    "        _fields_ = ((\"ki\", KEYBDINPUT),\n",
    "                    (\"mi\", MOUSEINPUT),\n",
    "                    (\"hi\", HARDWAREINPUT))\n",
    "    _anonymous_ = (\"_input\",)\n",
    "    _fields_ = ((\"type\",   wintypes.DWORD),\n",
    "                (\"_input\", _INPUT))\n",
    "\n",
    "def _check_count(result, func, args):\n",
    "    if result == 0:\n",
    "        raise ctypes.WinError(ctypes.get_last_error())\n",
    "    return args\n",
    "\n",
    "class InputHandler:\n",
    "\n",
    "    LPINPUT = ctypes.POINTER(INPUT)\n",
    "    user32.SendInput.errcheck = _check_count\n",
    "    user32.SendInput.argtypes = (wintypes.UINT, # nInputs\n",
    "                                 LPINPUT,       # pInputs\n",
    "                                 ctypes.c_int)  # cbSize\n",
    "\n",
    "    def __init__(self):\n",
    "        self.PS4RemotePlayHWND = 0\n",
    "        self.PS4RemotePlayPID = 0\n",
    "# Functions\n",
    "\n",
    "    def get_actions(self, amount):\n",
    "        actions = []\n",
    "        actions.append([])\n",
    "        action = 0\n",
    "        for i in range(0,amount):\n",
    "            temp = random.randint(0,1)\n",
    "            if temp == 0:\n",
    "                # select something random from the direction arroy\n",
    "                action = direction[random.randint(0,len(direction)-1)]\n",
    "            else:\n",
    "                # select something random from the attack array\n",
    "                action = attack[random.randint(0,len(direction)-1)]\n",
    "            # Only add the input if it is not 0. 0 Is the same as nothing.\n",
    "            if action != 0:\n",
    "                actions.append(action)\n",
    "        # Get the delay time for pressing these keys\n",
    "        delayVal = delay[random.randint(0,1)]\n",
    "        if delayVal == 'hold':\n",
    "            # can't use i as the index because I am only adding non 0 inputs\n",
    "            actions[0].append(random.uniform(0.04, 0.06))\n",
    "        else:\n",
    "            # can't use i as the index because I am only adding non 0 inputs\n",
    "            actions[0].append(random.uniform(0.02, 0.04))\n",
    "        return actions\n",
    "\n",
    "    def get_action(self, index):\n",
    "        print('Valid Actions:',valid_actions[index])\n",
    "        return valid_actions[index]\n",
    "\n",
    "    def get_remote_play_pid(self):\n",
    "        # register winapi functions\n",
    "        EnumWindows = ctypes.windll.user32.EnumWindows\n",
    "        EnumWindowsProc = ctypes.WINFUNCTYPE(ctypes.c_bool, ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int))\n",
    "        GetWindowText = ctypes.windll.user32.GetWindowTextW\n",
    "        GetWindowTextLength = ctypes.windll.user32.GetWindowTextLengthW\n",
    "        IsWindowVisible = ctypes.windll.user32.IsWindowVisible\n",
    "        GetWindowThreadProcessId = ctypes.windll.user32.GetWindowThreadProcessId\n",
    "\n",
    "        def foreach_window(self, hwnd, lParam):\n",
    "            # window must be visible\n",
    "            if IsWindowVisible(hwnd):\n",
    "                length = GetWindowTextLength(hwnd)\n",
    "                buff = ctypes.create_unicode_buffer(length + 1)\n",
    "                GetWindowText(hwnd, buff, length + 1)\n",
    "                try:\n",
    "                    windowtitle = buff.value\n",
    "                    if \"REPL4Y\" in windowtitle:\n",
    "                        # get the processid from the hwnd\n",
    "                        # declaring this as global means refer to the global version\n",
    "                        #global global_PS4RemotePlayPID\n",
    "                        #global global_PS4RemotePlayHWND\n",
    "                        processID = c_int()\n",
    "                        threadID = GetWindowThreadProcessId(hwnd, byref(processID))\n",
    "                        # found the process ID\n",
    "                        self.PS4RemotePlayPID = processID\n",
    "                        self.PS4RemotePlayHWND = hwnd\n",
    "                        return True\n",
    "                except:\n",
    "                    print(\"Unexpected error:\"+sys.exc_info()[0])\n",
    "                    pass;\n",
    "            return True\n",
    "        EnumWindows(EnumWindowsProc(foreach_window(self)), 0)\n",
    "\n",
    "\n",
    "    def press_key(self, hexKeyCode):\n",
    "        print(f\"Pressing key: {hexKeyCode}\")\n",
    "        x = INPUT(type=INPUT_KEYBOARD,\n",
    "                  ki=KEYBDINPUT(wVk=hexKeyCode))\n",
    "        user32.SendInput(1, ctypes.byref(x), ctypes.sizeof(x))\n",
    "\n",
    "    def release_key(self, hexKeyCode):\n",
    "        print(f\"Releasing key: {hexKeyCode}\")\n",
    "        x = INPUT(type=INPUT_KEYBOARD,\n",
    "                  ki=KEYBDINPUT(wVk=hexKeyCode,\n",
    "                                dwFlags=KEYEVENTF_KEYUP))\n",
    "        user32.SendInput(1, ctypes.byref(x), ctypes.sizeof(x))\n",
    "\n",
    "    def focus_window(self, hwnd):\n",
    "        ctypes.windll.user32.SetForegroundWindow(hwnd)\n",
    "        activate_remap()\n",
    "\n",
    "    def activate_remap(self):\n",
    "        time.sleep(0.5)\n",
    "        self.press_key(VK_LCONTROL)\n",
    "        self.press_key(VK_LMENU)\n",
    "        time.sleep(0.01)\n",
    "        self.release_key(VK_LCONTROL)\n",
    "        self.release_key(VK_LMENU)\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "    def hold_delay(self):\n",
    "        time.sleep(random.uniform(0.3, 0.35))\n",
    "\n",
    "    # def quick_press_delay(self):\n",
    "    #     time.sleep(random.uniform(0.03, 0.05))\n",
    "\n",
    "    def process_keys(self, keys, action_type):\n",
    "        \"\"\"Recursively process key presses and releases.\"\"\"\n",
    "        if isinstance(keys, tuple):\n",
    "            for key in keys:\n",
    "                self.process_keys(key, action_type)\n",
    "        else:\n",
    "            if keys != 0:\n",
    "                if action_type == 'press':\n",
    "                    self.press_key(keys)\n",
    "                elif action_type == 'release':\n",
    "                    self.release_key(keys)\n",
    "\n",
    "    def execute_action(self, actionIndex):\n",
    "        action = valid_actions[actionIndex]\n",
    "        print(f\"Executing action: {action}\")  # Debug print\n",
    "        self.press_key(88)\n",
    "        if(actionIndex==0): #(82, 89, 68, 68, 89, 68, 84)\n",
    "            self.press_key(88)\n",
    "            self.press_key(82)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(82)\n",
    "            self.press_key(89)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(89)\n",
    "            self.press_key(68)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(68)\n",
    "            self.press_key(68)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.press_key(89)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(89)\n",
    "            self.press_key(68)\n",
    "            time.sleep(random.uniform(0.03, 0.05))\n",
    "            self.release_key(68)\n",
    "            self.press_key(68)\n",
    "            self.press_key(84)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(84)\n",
    "        # Check if action is a tuple or list\n",
    "        elif isinstance(action, (tuple, list)):\n",
    "            self.press_key(88)\n",
    "            for key_sequence in action:\n",
    "                # Press keys in the current sequence\n",
    "                self.process_keys(key_sequence, 'press')\n",
    "                # Hold delay after pressing keys\n",
    "                self.hold_delay()\n",
    "                # Release keys in the current sequence\n",
    "                self.process_keys(key_sequence, 'release')\n",
    "        else:\n",
    "            # Handle single key press action\n",
    "            if action != 0:\n",
    "                self.process_keys(action, 'press')\n",
    "                self.hold_delay()\n",
    "                self.process_keys(action, 'release')\n",
    "            else:\n",
    "                print(f\"Invalid action structure: {action}\")\n",
    "        self.release_key(88)\n",
    "\n",
    "    def execute_actions(self, actions):\n",
    "        printf(\"in execute_Actions\",actions)\n",
    "        for action in actions:\n",
    "            self.execute_action(action)\n",
    "            # time.sleep(random.uniform(0.01, 0.03))\n",
    "\n",
    "    def choose_random_inputs():\n",
    "        x = 100\n",
    "        while x > 0:\n",
    "            amount = random.randint(0,7)\n",
    "            actions = get_actions(amount=amount)\n",
    "            execute_actions(actions=actions)\n",
    "            x = x-1\n",
    "    \n",
    "# This SumTree is used to create the prioritized experience replay for the DQQN.\n",
    "\n",
    "import numpy\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])    \n",
    "    \n",
    "\n",
    "IMAGE_STACK = 2\n",
    "IMAGE_WIDTH = 84\n",
    "IMAGE_HEIGHT = 84\n",
    "CHANNELS = 1\n",
    "HUBER_LOSS = 2.0\n",
    "LEARNING_RATE = 0.00025\n",
    "def huber_loss(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    cond = tf.abs(error) < HUBER_LOSS\n",
    "\n",
    "    squared_loss = 0.5 * tf.square(error)\n",
    "    linear_loss = HUBER_LOSS * (tf.abs(error) - 0.5 * HUBER_LOSS)\n",
    "\n",
    "    loss = tf.where(cond, squared_loss, linear_loss)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, input_shape, actionCnt, model=None, target_model=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        if model is not None: \n",
    "            self.model = model\n",
    "            self.target_model = target_model\n",
    "        else:\n",
    "            self.model = self._createModel()\n",
    "            self.target_model = self._createModel()\n",
    "\n",
    "    def _createModel(self):\n",
    "        model = Sequential()\n",
    "        print(\"Input Shape:\", self.input_shape)\n",
    "        print(\"Input Shape:\", self.actionCnt)\n",
    "        model.add(Conv2D(64, (2, 2), strides=(1, 1), activation='relu', padding='SAME',\n",
    "                         input_shape=self.input_shape, data_format='channels_last'))\n",
    "\n",
    "        model.add(Conv2D(32, (2,2), strides=(1,1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=218, activation='relu'))\n",
    "        model.add(Dense(units=self.actionCnt, activation='linear'))\n",
    "\n",
    "        opt = RMSprop(learning_rate=LEARNING_RATE)\n",
    "        model.compile(loss=huber_loss, optimizer=opt)\n",
    "        print(\"Before model.summary()\")\n",
    "        model.summary()\n",
    "        print(\"After model.summary()\")\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epochs=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=32, epochs=epochs, verbose=verbose)\n",
    "        \n",
    "    def predict(self, s, target=False):\n",
    "        if target:\n",
    "            return self.target_model.predict(s)\n",
    "        else:\n",
    "            return self.model.predict(s)\n",
    "\n",
    "    def predict_one(self, s, target=False):\n",
    "        return self.predict(s.reshape(1, IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT), target).flatten()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "MEMORY_CAPACITY = 400000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "\n",
    "EXPLORATION_STOP = 500000\n",
    "LAMBDA = - math.log(0.01) / EXPLORATION_STOP\n",
    "\n",
    "UPDATE_TARGET_FREQUENCY = 10000\n",
    "\n",
    "class LearningAgent:\n",
    "    steps = 0\n",
    "    latest_Q = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self, learning=False, epsilon=1.0, alpha=0.5):\n",
    "        #super(LearningAgent, self).init()\n",
    "        self.input_handler = InputHandler()\n",
    "        self.learning = learning\n",
    "        self.inputShape = (IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        self.numActions = len(valid_actions)\n",
    "        self.model = Model(self.inputShape, self.numActions)\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        #self.valid_actions = []\n",
    "    def observe(self, sample): #(s, a, r, s_)\n",
    "        x, y, errors = self.get_targets([(0, sample)])\n",
    "        self.memory.add(errors[0], sample)\n",
    "\n",
    "        if self.steps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            self.model.update_target_model()\n",
    "\n",
    "        self.steps = self.steps + 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "    def get_targets(self, batch):\n",
    "        no_state = np.zeros(self.inputShape)\n",
    "\n",
    "        states = np.array([o[1][0] for o in batch])\n",
    "        states_ = np.array([(no_state if o[1][3] is None else o[1][3]) for o in batch])\n",
    "\n",
    "        # p is the prediction for a given state\n",
    "        p = agent.model.predict(states)\n",
    "\n",
    "        p_ = agent.model.predict(states_, target=False) # target set to false means predict from model\n",
    "        pTarget_ = agent.model.predict(states_, target=True) # target true means predict from target model\n",
    "\n",
    "        x = np.zeros((len(batch), IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        y = np.zeros((len(batch), self.numActions))\n",
    "        errors = np.zeros(len(batch))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            o = batch[i][1]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            t = p[i]\n",
    "            oldVal = t[a]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * pTarget_[i][np.argmax(p_[i])] # DDQN portion\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "            errors[i] = abs(oldVal - t[a])\n",
    "            self.latest_Q = t[a]\n",
    "\n",
    "        return (x, y, errors)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # call when the agent must make a decision based on the state\n",
    "        self.state = state\n",
    "        self.steps += 1\n",
    "        action = None\n",
    "\n",
    "        if self.learning == False:\n",
    "            #action = self.input_handler.get_action(random.randint(0,len(valid_actions)-1))\n",
    "            action = random.randint(0, len(valid_actions)-1)\n",
    "        else:\n",
    "            if random.uniform(0,1) < self.epsilon:\n",
    "                #action = self.input_handler.get_action(random.randint(0,len(valid_actions)-1))\n",
    "                action = random.randint(0, len(valid_actions)-1)\n",
    "                print(\"in if valid_actions\",len(valid_actions))\n",
    "            else:\n",
    "                action = np.argmax(self.model.predict_one(state))\n",
    "                print(\"in else valid_actions\",len(valid_actions))\n",
    "        return action\n",
    "\n",
    "    def execute_action(self, action):\n",
    "        self.input_handler.execute_action(action)\n",
    "\n",
    "    def replay(self):\n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        x, y, errors = self.get_targets(batch)\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            idx = batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        self.model.train(x, y)\n",
    "\n",
    "    def play(self, state):\n",
    "        self.state = state\n",
    "        action = np.argmax(self.model.predict_one(state))\n",
    "        return action\n",
    "class Vision:\n",
    "    screen = {'top': 110, 'left': 0, 'width': 1920, 'height': 970}\n",
    "    leftHPCapture = {'top': 110, 'left': 240, 'width': 620, 'height': 20}\n",
    "    rightHPCapture = {'top': 110, 'left': 1043, 'width': 620, 'height': 20}\n",
    "    \n",
    "    positive = 1    # AI hit the opponent\n",
    "    negative = -1   # AI took a hit\n",
    "\n",
    "    def __init__(self, side):\n",
    "        self.side = side\n",
    "        with mss.mss() as sct:\n",
    "            self.prevLeftHP = self.numpy_img_to_gray(np.array(sct.grab(self.leftHPCapture)))\n",
    "            self.prevRightHP = self.numpy_img_to_gray(np.array(sct.grab(self.rightHPCapture)))\n",
    "\n",
    "# Dot product to reduce the pixel values to their grayscale equivlalent\n",
    "    def numpy_img_to_gray(self, img):\n",
    "        return np.dot(img[...,:3], [0.299,0.587,0.114])\n",
    "\n",
    "    def get_current_screen(self):\n",
    "        with mss.mss() as sct:\n",
    "            sct_img = sct.grab(self.screen)\n",
    "            img = Image.frombytes('RGB', sct_img.size, sct_img.rgb)\n",
    "            img = img.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)\n",
    "            currScreen = np.array(img)\n",
    "        return self.numpy_img_to_gray(currScreen)\n",
    "\n",
    "    def get_reward(self):\n",
    "        with mss.mss() as sct:\n",
    "            reward = 0\n",
    "\n",
    "            currLeftHP = np.array(sct.grab(self.leftHPCapture))\n",
    "            currRightHP = np.array(sct.grab(self.rightHPCapture))\n",
    "            # Convert to gray\n",
    "            currLeftHP = self.numpy_img_to_gray(currLeftHP)\n",
    "            currRightHP = self.numpy_img_to_gray(currRightHP)\n",
    "            # get the difference in previous vs current\n",
    "            diffLeftHP = self.prevLeftHP - currLeftHP\n",
    "            diffRightHP = self.prevRightHP - currRightHP\n",
    "            # round negative values up to 0\n",
    "            diffLeftHP = diffLeftHP.clip(min=0)\n",
    "            diffRightHP = diffRightHP.clip(min=0)\n",
    "            # If hit, there are typically more than 10 pixels with values > 120\n",
    "            if((diffLeftHP > 125).sum() > 10):\n",
    "                if(self.side == 'left'):\n",
    "                    reward = reward - 1\n",
    "                else:\n",
    "                    reward = reward + 1\n",
    "            # If hit, there are typically more than 10 pixels with values > 120\n",
    "            if((diffRightHP > 125).sum() > 10):\n",
    "                if(self.side == 'right'):\n",
    "                    reward = reward - 1\n",
    "                else:\n",
    "                    reward = reward + 1\n",
    "            # Set previous frame data to current frame data\n",
    "            self.prevLeftHP = currLeftHP\n",
    "            self.prevRightHP = currRightHP\n",
    "\n",
    "        return reward\n",
    "\n",
    "# SumTree of previous decisions\n",
    "class Memory:\n",
    "    def __init__(self, capacity, epsilon=1.0, alpha=0.5):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append((idx, data))\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "def import_model(agent):\n",
    "    try:\n",
    "        agent.model = Model(agent.inputShape, agent.numActions,\n",
    "            load_model('model backup/trial 2/TekkenBotDDQN_1.h5', custom_objects={'huber_loss':huber_loss}),\n",
    "                            load_model('model backup/trial 2/TekkenBotDDQN_Target_1.h5', custom_objects={'huber_loss':huber_loss}))\n",
    "        print('Model loaded for agent.')\n",
    "    except Exception as e:\n",
    "        print('Model failed to load.')\n",
    "        print(e)\n",
    "\n",
    "# Used when not learning.\n",
    "def play(agent):\n",
    "    agent.input_handler.activate_remap()\n",
    "    vision = Vision('left')\n",
    "    w = vision.get_current_screen()\n",
    "    state = np.array([w,w])\n",
    "    try:\n",
    "        while True:\n",
    "            actionIndex = agent.play(state)\n",
    "            agent.execute_action(actionIndex)\n",
    "            screenCap = vision.get_current_screen()\n",
    "            statePrime = np.array((state[1], screenCap))\n",
    "            state = statePrime\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('break')\n",
    "\n",
    "\n",
    "TRIAL = 1 # necessary for correct model backup\n",
    "TOTAL_TESTS = 1 # 3 12 hour segments of testing\n",
    "TOTAL_EPISODES =  100 # 720 = 12 hours\n",
    "def save_models(agent, curr_test):\n",
    "\n",
    "    directory = 'model backup/trial 2'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + \"/TekkenBotDDQN_1.h5\"\n",
    "    agent.model.model.save(filename, overwrite=True)\n",
    "    print(\"Model saved as: TekkenBotDDQN_1.h5\")\n",
    "\n",
    "    filename = directory + \"/TekkenBotDDQN_Target_1.h5\"\n",
    "    agent.model.target_model.save(filename, overwrite=True)\n",
    "    print(\"Target Model saved as: TekkenBotDDQN_Target_1.h5\")\n",
    "\n",
    "# lets start the show. Used when learning.\n",
    "def run(agent):\n",
    "    agent.input_handler.activate_remap()\n",
    "    vision = Vision('left')\n",
    "    w = vision.get_current_screen()\n",
    "    state = np.array([w, w])\n",
    "    start_time = time.time()\n",
    "\n",
    "    rewardTotal = 0\n",
    "    episode = 0\n",
    "    true_episode = 0\n",
    "    curr_test = 1\n",
    "    record_size = TOTAL_EPISODES * TOTAL_TESTS + 1\n",
    "    reward_per_episode = np.zeros((record_size, 4))\n",
    "    i = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            actionIndex = agent.choose_action(state)\n",
    "            agent.execute_action(actionIndex)\n",
    "            screenCap = vision.get_current_screen()\n",
    "            reward = vision.get_reward()\n",
    "\n",
    "            statePrime = np.array((state[1], screenCap))\n",
    "\n",
    "            agent.observe((state, actionIndex, reward, statePrime))\n",
    "            agent.replay()\n",
    "\n",
    "            state = statePrime\n",
    "            rewardTotal += reward\n",
    "\n",
    "            if time.time() - start_time > 59:\n",
    "                reward_per_episode[i] = [true_episode, rewardTotal, agent.steps, agent.latest_Q]\n",
    "                print(\"Episode {} Ended. Reward earned: {}\".format(episode, rewardTotal))\n",
    "                episode += 1\n",
    "                true_episode += 1\n",
    "                rewardTotal = 0\n",
    "                i += 1\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Perform evaluation every 10 episodes (or any interval you prefer)\n",
    "                if episode % 2 == 0:#10\n",
    "                    evaluate(agent, num_episodes=2)#10\n",
    "\n",
    "            if episode >= TOTAL_EPISODES:\n",
    "                if curr_test >= TOTAL_TESTS:\n",
    "                    save_models(agent, curr_test)\n",
    "                    agent.input_handler.activate_remap()\n",
    "                    raise KeyboardInterrupt('Episodes limit reached')\n",
    "                else:\n",
    "                    save_models(agent, curr_test)\n",
    "                    curr_test += 1\n",
    "                    episode = 0\n",
    "                    start_time = time.time()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        np.savetxt('model backup/trial 2/episodesAndRewards.txt'.format(TRIAL), reward_per_episode, fmt='%d')\n",
    "        print(\"Episodes and rewards saved to episodesAndRewards.txt\")\n",
    "        save_models(agent, curr_test)\n",
    "        \n",
    "def evaluate(agent, num_episodes=2):#10\n",
    "    agent.input_handler.activate_remap()\n",
    "    vision = Vision('left')\n",
    "    rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs = vision.get_current_screen()\n",
    "        state = np.array([obs, obs])\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        start_time = time.time()  # Initialize start_time for each episode\n",
    "\n",
    "        while not done:\n",
    "            actionIndex = agent.choose_action(state)\n",
    "            agent.execute_action(actionIndex)\n",
    "            screenCap = vision.get_current_screen()\n",
    "            reward = vision.get_reward()\n",
    "\n",
    "            statePrime = np.array((state[1], screenCap))\n",
    "            state = statePrime\n",
    "\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            # Determine the end of an episode based on your specific conditions\n",
    "            if time.time() - start_time > 59:  # Or other episode-ending conditions\n",
    "                done = True\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    mean_length = np.mean(episode_lengths)\n",
    "    std_length = np.std(episode_lengths)\n",
    "\n",
    "    print(f'Evaluation over {num_episodes} episodes:')\n",
    "    print(f'Average Reward: {mean_reward} ± {std_reward}')\n",
    "    print(f'Average Episode Length: {mean_length} ± {std_length}')\n",
    "    \n",
    "    return rewards, episode_lengths\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        agent = LearningAgent(learning=True, epsilon=MAX_EPSILON, alpha=LEARNING_RATE)\n",
    "        import_model(agent)\n",
    "        run(agent)\n",
    "        # play(agent)\n",
    "    finally:\n",
    "        print('Thanks for playing!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
