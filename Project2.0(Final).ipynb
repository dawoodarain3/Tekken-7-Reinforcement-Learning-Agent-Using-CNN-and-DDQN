{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b889b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (2, 84, 84)\n",
      "Input Shape: 19\n",
      "Before model.summary()\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 2, 84, 64)         21568     \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 83, 32)         8224      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2656)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 218)               579226    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 19)                4161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 613,179\n",
      "Trainable params: 613,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "After model.summary()\n",
      "Input Shape: (2, 84, 84)\n",
      "Input Shape: 19\n",
      "Before model.summary()\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 2, 84, 64)         21568     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 83, 32)         8224      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2656)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 218)               579226    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 19)                4161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 613,179\n",
      "Trainable params: 613,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "After model.summary()\n",
      "Model loaded for agent.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow\n",
    "import time\n",
    "import cv2\n",
    "import mss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Input, BatchNormalization, Dropout, Add\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import sys\n",
    "import ctypes\n",
    "from ctypes import wintypes, c_int, byref\n",
    "import time, random\n",
    "from collections import deque\n",
    "\n",
    "user32 = ctypes.WinDLL('user32', use_last_error=True)\n",
    "\n",
    "INPUT_MOUSE    = 0\n",
    "INPUT_KEYBOARD = 1\n",
    "INPUT_HARDWARE = 2\n",
    "\n",
    "KEYEVENTF_EXTENDEDKEY = 0x0001\n",
    "KEYEVENTF_KEYUP       = 0x0002\n",
    "KEYEVENTF_UNICODE     = 0x0004\n",
    "KEYEVENTF_SCANCODE    = 0x0008\n",
    "\n",
    "MAPVK_VK_TO_VSC = 0\n",
    "\n",
    "# msdn.microsoft.com/en-us/library/dd375731\n",
    "VK_LMENU    = 0x12 # Left Alt key 18\n",
    "VK_LCONTROL = 0xA2 # Left Ctrl key 162\n",
    "\n",
    "# Playstation valid buttons\n",
    "DPAD_UP     = 0x57 # W key 87\n",
    "DPAD_DOWN   = 0x53 # S key 83\n",
    "DPAD_LEFT   = 0x41 # A key 65\n",
    "DPAD_RIGHT  = 0x44 # D key 68\n",
    "CROSS       = 0x45 # E key 69\n",
    "SQUARE      = 0x52 # R key 82\n",
    "CIRCLE      = 0x54 # T key 84\n",
    "TRIANGLE    = 0x59 # Y key 89\n",
    "R1          = 0x51 # Q key 81\n",
    "TOUCHPAD    = 0x58 # X l1 key 88\n",
    "\n",
    "# Diagonal directional buttons with arbitrary values\n",
    "DIAG_DOWN_LEFT = (DPAD_LEFT, DPAD_DOWN)\n",
    "DIAG_DOWN_RIGHT = (DPAD_RIGHT, DPAD_DOWN)\n",
    "DIAG_UP_LEFT = (DPAD_LEFT, DPAD_UP)\n",
    "DIAG_UP_RIGHT = (DPAD_RIGHT, DPAD_UP)\n",
    "\n",
    "# Multi button attacks with arbitrary values\n",
    "CROSS_SQUARE = (0,(CROSS, SQUARE))\n",
    "CROSS_CIRCLE = (TRIANGLE,TRIANGLE,TRIANGLE,TRIANGLE)\n",
    "SQUARE_TRIANGLE = (CROSS,TRIANGLE)\n",
    "TRIANGLE_CIRCLE = (0,(TRIANGLE, CIRCLE))\n",
    "\n",
    "ATTACK01 = (SQUARE,TRIANGLE,DPAD_RIGHT,DPAD_RIGHT,TRIANGLE,DPAD_RIGHT,CIRCLE)\n",
    "\n",
    "delay = ['hold','tap']\n",
    "# available inputs by type\n",
    "direction = [0, DPAD_UP, DPAD_DOWN, DPAD_LEFT, DPAD_RIGHT, DIAG_DOWN_LEFT, DIAG_DOWN_RIGHT, DIAG_UP_LEFT, DIAG_UP_RIGHT]\n",
    "attack = [ATTACK01,TRIANGLE, CIRCLE, CROSS, SQUARE,0, DPAD_UP, DPAD_DOWN, DPAD_LEFT, DPAD_RIGHT, DIAG_DOWN_LEFT, DIAG_DOWN_RIGHT, DIAG_UP_LEFT, DIAG_UP_RIGHT,\n",
    "           CROSS_SQUARE, CROSS_CIRCLE, SQUARE_TRIANGLE, TRIANGLE_CIRCLE,R1,]\n",
    "rage = [R1]\n",
    "\n",
    "# all valid actions\n",
    "valid_actions = attack\n",
    "# valid_actions = [(x,y) for x in direction for y in attack]\n",
    "\n",
    "wintypes.ULONG_PTR = wintypes.WPARAM\n",
    "\n",
    "class MOUSEINPUT(ctypes.Structure):\n",
    "    _fields_ = ((\"dx\",          wintypes.LONG),\n",
    "                (\"dy\",          wintypes.LONG),\n",
    "                (\"mouseData\",   wintypes.DWORD),\n",
    "                (\"dwFlags\",     wintypes.DWORD),\n",
    "                (\"time\",        wintypes.DWORD),\n",
    "                (\"dwExtraInfo\", wintypes.ULONG_PTR))\n",
    "\n",
    "class KEYBDINPUT(ctypes.Structure):\n",
    "    _fields_ = ((\"wVk\",         wintypes.WORD),\n",
    "                (\"wScan\",       wintypes.WORD),\n",
    "                (\"dwFlags\",     wintypes.DWORD),\n",
    "                (\"time\",        wintypes.DWORD),\n",
    "                (\"dwExtraInfo\", wintypes.ULONG_PTR))\n",
    "\n",
    "    def __init__(self, *args, **kwds):\n",
    "        super(KEYBDINPUT, self).__init__(*args, **kwds)\n",
    "        # some programs use the scan code even if KEYEVENTF_SCANCODE\n",
    "        # isn't set in dwFflags, so attempt to map the correct code.\n",
    "        if not self.dwFlags & KEYEVENTF_UNICODE:\n",
    "            self.wScan = user32.MapVirtualKeyExW(self.wVk,\n",
    "                                                 MAPVK_VK_TO_VSC, 0)\n",
    "class HARDWAREINPUT(ctypes.Structure):\n",
    "    _fields_ = ((\"uMsg\",    wintypes.DWORD),\n",
    "                (\"wParamL\", wintypes.WORD),\n",
    "                (\"wParamH\", wintypes.WORD))\n",
    "\n",
    "class INPUT(ctypes.Structure):\n",
    "    class _INPUT(ctypes.Union):\n",
    "        _fields_ = ((\"ki\", KEYBDINPUT),\n",
    "                    (\"mi\", MOUSEINPUT),\n",
    "                    (\"hi\", HARDWAREINPUT))\n",
    "    _anonymous_ = (\"_input\",)\n",
    "    _fields_ = ((\"type\",   wintypes.DWORD),\n",
    "                (\"_input\", _INPUT))\n",
    "\n",
    "def _check_count(result, func, args):\n",
    "    if result == 0:\n",
    "        raise ctypes.WinError(ctypes.get_last_error())\n",
    "    return args\n",
    "\n",
    "class InputHandler:\n",
    "    LPINPUT = ctypes.POINTER(INPUT)\n",
    "    user32.SendInput.errcheck = _check_count\n",
    "    user32.SendInput.argtypes = (wintypes.UINT, # nInputs\n",
    "                                 LPINPUT,       # pInputs\n",
    "                                 ctypes.c_int)  # cbSize\n",
    "\n",
    "    def __init__(self):\n",
    "        self.PS4RemotePlayHWND = 0\n",
    "        self.PS4RemotePlayPID = 0\n",
    "        # Create a command buffer to optimize action execution\n",
    "        self.command_buffer = []\n",
    "        \n",
    "    def queue_command(self, key, action_type, delay=0):\n",
    "        \"\"\"Queue a command to execute later\"\"\"\n",
    "        self.command_buffer.append((key, action_type, delay))\n",
    "        \n",
    "    def execute_command_buffer(self):\n",
    "        \"\"\"Execute all queued commands in sequence\"\"\"\n",
    "        for key, action_type, delay in self.command_buffer:\n",
    "            if action_type == 'press':\n",
    "                self.press_key(key)\n",
    "            elif action_type == 'release':\n",
    "                self.release_key(key)\n",
    "            \n",
    "            if delay > 0:\n",
    "                time.sleep(delay)\n",
    "                \n",
    "        # Clear buffer after execution\n",
    "        self.command_buffer = []\n",
    "\n",
    "    def get_actions(self, amount):\n",
    "        actions = []\n",
    "        actions.append([])\n",
    "        action = 0\n",
    "        for i in range(0,amount):\n",
    "            temp = random.randint(0,1)\n",
    "            if temp == 0:\n",
    "                # select something random from the direction arroy\n",
    "                action = direction[random.randint(0,len(direction)-1)]\n",
    "            else:\n",
    "                # select something random from the attack array\n",
    "                action = attack[random.randint(0,len(direction)-1)]\n",
    "            # Only add the input if it is not 0. 0 Is the same as nothing.\n",
    "            if action != 0:\n",
    "                actions.append(action)\n",
    "        # Get the delay time for pressing these keys\n",
    "        delayVal = delay[random.randint(0,1)]\n",
    "        if delayVal == 'hold':\n",
    "            # can't use i as the index because I am only adding non 0 inputs\n",
    "            actions[0].append(random.uniform(0.04, 0.06))\n",
    "        else:\n",
    "            # can't use i as the index because I am only adding non 0 inputs\n",
    "            actions[0].append(random.uniform(0.02, 0.04))\n",
    "        return actions\n",
    "\n",
    "    def get_action(self, index):\n",
    "        print('Valid Actions:', valid_actions[index])\n",
    "        return valid_actions[index]\n",
    "\n",
    "    def get_remote_play_pid(self):\n",
    "        # register winapi functions\n",
    "        EnumWindows = ctypes.windll.user32.EnumWindows\n",
    "        EnumWindowsProc = ctypes.WINFUNCTYPE(ctypes.c_bool, ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int))\n",
    "        GetWindowText = ctypes.windll.user32.GetWindowTextW\n",
    "        GetWindowTextLength = ctypes.windll.user32.GetWindowTextLengthW\n",
    "        IsWindowVisible = ctypes.windll.user32.IsWindowVisible\n",
    "        GetWindowThreadProcessId = ctypes.windll.user32.GetWindowThreadProcessId\n",
    "\n",
    "        def foreach_window(self, hwnd, lParam):\n",
    "            # window must be visible\n",
    "            if IsWindowVisible(hwnd):\n",
    "                length = GetWindowTextLength(hwnd)\n",
    "                buff = ctypes.create_unicode_buffer(length + 1)\n",
    "                GetWindowText(hwnd, buff, length + 1)\n",
    "                try:\n",
    "                    windowtitle = buff.value\n",
    "                    if \"REPL4Y\" in windowtitle:\n",
    "                        # get the processid from the hwnd\n",
    "                        # declaring this as global means refer to the global version\n",
    "                        processID = c_int()\n",
    "                        threadID = GetWindowThreadProcessId(hwnd, byref(processID))\n",
    "                        # found the process ID\n",
    "                        self.PS4RemotePlayPID = processID\n",
    "                        self.PS4RemotePlayHWND = hwnd\n",
    "                        return True\n",
    "                except:\n",
    "                    print(\"Unexpected error:\"+sys.exc_info()[0])\n",
    "                    pass;\n",
    "            return True\n",
    "        EnumWindows(EnumWindowsProc(foreach_window(self)), 0)\n",
    "\n",
    "    def press_key(self, hexKeyCode):\n",
    "        print(f\"Pressing key: {hexKeyCode}\")\n",
    "        x = INPUT(type=INPUT_KEYBOARD,\n",
    "                  ki=KEYBDINPUT(wVk=hexKeyCode))\n",
    "        user32.SendInput(1, ctypes.byref(x), ctypes.sizeof(x))\n",
    "\n",
    "    def release_key(self, hexKeyCode):\n",
    "        print(f\"Releasing key: {hexKeyCode}\")\n",
    "        x = INPUT(type=INPUT_KEYBOARD,\n",
    "                  ki=KEYBDINPUT(wVk=hexKeyCode,\n",
    "                                dwFlags=KEYEVENTF_KEYUP))\n",
    "        user32.SendInput(1, ctypes.byref(x), ctypes.sizeof(x))\n",
    "\n",
    "    def focus_window(self, hwnd):\n",
    "        ctypes.windll.user32.SetForegroundWindow(hwnd)\n",
    "        self.activate_remap()\n",
    "\n",
    "    def activate_remap(self):\n",
    "        time.sleep(0.5)\n",
    "        self.press_key(VK_LCONTROL)\n",
    "        self.press_key(VK_LMENU)\n",
    "        time.sleep(0.01)\n",
    "        self.release_key(VK_LCONTROL)\n",
    "        self.release_key(VK_LMENU)\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "    def hold_delay(self):\n",
    "        time.sleep(random.uniform(0.3, 0.35))\n",
    "\n",
    "    def process_keys(self, keys, action_type):\n",
    "        \"\"\"Recursively process key presses and releases.\"\"\"\n",
    "        if isinstance(keys, tuple):\n",
    "            for key in keys:\n",
    "                self.process_keys(key, action_type)\n",
    "        else:\n",
    "            if keys != 0:\n",
    "                if action_type == 'press':\n",
    "                    self.press_key(keys)\n",
    "                elif action_type == 'release':\n",
    "                    self.release_key(keys)\n",
    "\n",
    "    def execute_action(self, actionIndex):\n",
    "        action = valid_actions[actionIndex]\n",
    "        print(f\"Executing action: {action}\")  # Debug print\n",
    "        self.press_key(88)\n",
    "        if(actionIndex==0): #(82, 89, 68, 68, 89, 68, 84)\n",
    "            self.press_key(88)\n",
    "            self.press_key(82)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(82)\n",
    "            self.press_key(89)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(89)\n",
    "            self.press_key(68)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(68)\n",
    "            self.press_key(68)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.press_key(89)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(89)\n",
    "            self.press_key(68)\n",
    "            time.sleep(random.uniform(0.03, 0.05))\n",
    "            self.release_key(68)\n",
    "            self.press_key(68)\n",
    "            self.press_key(84)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(84)\n",
    "        # Check if action is a tuple or list\n",
    "        elif isinstance(action, (tuple, list)):\n",
    "            self.press_key(88)\n",
    "            for key_sequence in action:\n",
    "                # Press keys in the current sequence\n",
    "                self.process_keys(key_sequence, 'press')\n",
    "                # Hold delay after pressing keys\n",
    "                self.hold_delay()\n",
    "                # Release keys in the current sequence\n",
    "                self.process_keys(key_sequence, 'release')\n",
    "        else:\n",
    "            # Handle single key press action\n",
    "            if action != 0:\n",
    "                self.process_keys(action, 'press')\n",
    "                self.hold_delay()\n",
    "                self.process_keys(action, 'release')\n",
    "            else:\n",
    "                print(f\"Invalid action structure: {action}\")\n",
    "        self.release_key(88)\n",
    "\n",
    "    def execute_actions(self, actions):\n",
    "        print(\"in execute_Actions\", actions)\n",
    "        for action in actions:\n",
    "            self.execute_action(action)\n",
    "            # time.sleep(random.uniform(0.01, 0.03))\n",
    "\n",
    "# Improved SumTree implementation with more efficient operations\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "        self.n_entries = 0\n",
    "        \n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        \n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "            \n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        \n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "            \n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "        \n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "    \n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        \n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        \n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "            \n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "            \n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        \n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "        \n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        \n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        \"\"\"More efficient batch sampling\"\"\"\n",
    "        indices = []\n",
    "        priorities = []\n",
    "        data = []\n",
    "        segment = self.total() / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            \n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, item = self.get(s)\n",
    "            \n",
    "            indices.append(idx)\n",
    "            priorities.append(p)\n",
    "            data.append(item)\n",
    "            \n",
    "        return indices, priorities, data\n",
    "\n",
    "# IMPROVED HYPERPARAMETERS\n",
    "IMAGE_STACK = 4 \n",
    "IMAGE_WIDTH = 84\n",
    "IMAGE_HEIGHT = 84\n",
    "HUBER_LOSS = 1.0  # Reduced for more stable learning\n",
    "LEARNING_RATE = 0.0001  # Lower learning rate for better stability\n",
    "MEMORY_CAPACITY = 500000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "MAX_EPSILON = 1.0\n",
    "MIN_EPSILON = 0.05\n",
    "EXPLORATION_STOP = 500000  # Shorter exploration for faster convergence\n",
    "LAMBDA = -math.log(0.01) / EXPLORATION_STOP\n",
    "UPDATE_TARGET_FREQUENCY = 2000  # More frequent updates\n",
    "FRAME_SKIP = 2  # Process every nth frame for efficiency\n",
    "REPLAY_PERIOD = 4  # How often to perform replay\n",
    "\n",
    "# Tracking variables\n",
    "best_eval_reward = float('-inf')\n",
    "running_reward = 0\n",
    "\n",
    "def huber_loss(y_true, y_pred):\n",
    "    \"\"\"Huber loss function for robust training\"\"\"\n",
    "    error = y_true - y_pred\n",
    "    cond = tf.abs(error) < HUBER_LOSS\n",
    "    \n",
    "    squared_loss = 0.5 * tf.square(error)\n",
    "    linear_loss = HUBER_LOSS * (tf.abs(error) - 0.5 * HUBER_LOSS)\n",
    "    \n",
    "    loss = tf.where(cond, squared_loss, linear_loss)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"More efficient frame preprocessing with proper normalization\"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(frame.shape) > 2 and frame.shape[2] > 1:\n",
    "        gray = np.dot(frame[...,:3], [0.299, 0.587, 0.114])\n",
    "    else:\n",
    "        gray = frame\n",
    "    \n",
    "    # Resize efficiently\n",
    "    resized = cv2.resize(gray, (IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Normalize to [0,1] range\n",
    "    normalized = resized / 255.0\n",
    "    \n",
    "    return normalized.astype(np.float32)\n",
    "\n",
    "def create_state(frame):\n",
    "    \"\"\"Create a state by stacking the same frame multiple times\"\"\"\n",
    "    # Create an array of stacked identical frames, more efficient than stack\n",
    "    frames = [frame] * IMAGE_STACK\n",
    "    return np.array(frames).transpose(1, 2, 0)  # HWC format\n",
    "\n",
    "def update_state(state, new_frame):\n",
    "    \"\"\"Update state with a new frame - efficient implementation\"\"\"\n",
    "    # Update state by shifting frames and adding new frame\n",
    "    # This avoids creating a new array each time\n",
    "    state = np.roll(state, -1, axis=2)  # Roll along channel dimension\n",
    "    state[:, :, -1] = new_frame  # Add new frame\n",
    "    return state\n",
    "\n",
    "def augment_state(state):\n",
    "    \"\"\"Data augmentation for better generalization\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    augmented = np.copy(state)\n",
    "    \n",
    "    # Random brightness adjustment\n",
    "    if random.random() < 0.3:\n",
    "        brightness_factor = random.uniform(0.8, 1.2)\n",
    "        augmented = np.clip(augmented * brightness_factor, 0, 1.0)\n",
    "    \n",
    "    # Random contrast adjustment\n",
    "    if random.random() < 0.3:\n",
    "        contrast_factor = random.uniform(0.8, 1.2)\n",
    "        mean = np.mean(augmented)\n",
    "        augmented = np.clip((augmented - mean) * contrast_factor + mean, 0, 1.0)\n",
    "        \n",
    "    # Adding random noise - improves robustness\n",
    "    if random.random() < 0.2:\n",
    "        noise = np.random.normal(0, 0.01, augmented.shape)\n",
    "        augmented = np.clip(augmented + noise, 0, 1.0)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Residual block for better gradient flow\"\"\"\n",
    "    def __init__(self, filters, kernel_size=3, strides=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = Conv2D(filters, kernel_size, strides=strides, padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(filters, kernel_size, padding='same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        \n",
    "        # Skip connection\n",
    "        self.skip = Conv2D(filters, 1, strides=strides, padding='same') if strides > 1 else None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        # Apply skip connection\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inputs)\n",
    "        else:\n",
    "            skip = inputs\n",
    "            \n",
    "        x = tf.nn.relu(x + skip)\n",
    "        return x\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, input_shape, actionCnt, model=None, target_model=None, use_dueling=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.actionCnt = actionCnt\n",
    "        self.steps = 0\n",
    "        self.use_dueling = use_dueling\n",
    "        \n",
    "        # Setup TensorBoard with better logging\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.log_dir = f\"logs/fit/{current_time}\"\n",
    "        self.tensorboard_callback = TensorBoard(\n",
    "            log_dir=self.log_dir,\n",
    "            histogram_freq=1,\n",
    "            update_freq='epoch',\n",
    "            profile_batch=0\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.initial_learning_rate = LEARNING_RATE\n",
    "        \n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.target_model = target_model\n",
    "        else:\n",
    "            if use_dueling:\n",
    "                self.model = self._createDuelingModel()\n",
    "                self.target_model = self._createDuelingModel()\n",
    "                # Ensure target model has same weights\n",
    "                self.update_target_model()\n",
    "            else:\n",
    "                self.model = self._createModel()\n",
    "                self.target_model = self._createModel()\n",
    "                self.update_target_model()\n",
    "                \n",
    "    def _createModel(self):\n",
    "        \"\"\"Basic DQN model with residual connections\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input shape in HWC format - height, width, channels(frames)\n",
    "        input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_STACK)\n",
    "        \n",
    "        # First convolutional layer\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', \n",
    "                         input_shape=input_shape, padding='same'))\n",
    "        \n",
    "        # Add residual blocks for better gradient flow\n",
    "        model.add(ResidualBlock(64, 4, strides=2))\n",
    "        model.add(ResidualBlock(64, 3))\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=self.actionCnt, activation='linear'))\n",
    "        \n",
    "        # Use Adam optimizer with gradient clipping\n",
    "        opt = Adam(learning_rate=LEARNING_RATE, clipnorm=1.0)\n",
    "        model.compile(loss=huber_loss, optimizer=opt)\n",
    "        model.summary()\n",
    "        return model\n",
    "        \n",
    "    def _createDuelingModel(self):\n",
    "        \"\"\"Dueling DQN with residual connections\"\"\"\n",
    "        # Input in HWC format\n",
    "        input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_STACK)\n",
    "        input_layer = Input(shape=input_shape)\n",
    "        \n",
    "        # Convolutional layers with residual connections\n",
    "        conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='same')(input_layer)\n",
    "        res1 = ResidualBlock(64, 4, strides=2)(conv1)\n",
    "        res2 = ResidualBlock(64, 3)(res1)\n",
    "        \n",
    "        flat = Flatten()(res2)\n",
    "        \n",
    "        # Value stream (estimates state value)\n",
    "        value_fc = Dense(512, activation='relu')(flat)\n",
    "        value_dropout = Dropout(0.2)(value_fc)\n",
    "        value = Dense(1)(value_dropout)\n",
    "        \n",
    "        # Advantage stream (estimates action advantages)\n",
    "        adv_fc = Dense(512, activation='relu')(flat)\n",
    "        adv_dropout = Dropout(0.2)(adv_fc)\n",
    "        advantage = Dense(self.actionCnt)(adv_dropout)\n",
    "        \n",
    "        # Combine value and advantage (dueling architecture)\n",
    "        # Subtract mean to ensure identifiability\n",
    "        outputs = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
    "        \n",
    "        model = tf.keras.Model(inputs=input_layer, outputs=outputs)\n",
    "        opt = Adam(learning_rate=LEARNING_RATE, clipnorm=1.0)\n",
    "        model.compile(loss=huber_loss, optimizer=opt)\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def train(self, x, y, sample_weight=None, epochs=1, verbose=0):\n",
    "        \"\"\"Training with learning rate decay and callbacks\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Apply learning rate decay schedule\n",
    "        if self.steps > 50000:\n",
    "            lr = self.initial_learning_rate * 0.5\n",
    "        elif self.steps > 100000:\n",
    "            lr = self.initial_learning_rate * 0.25\n",
    "        elif self.steps > 200000:\n",
    "            lr = self.initial_learning_rate * 0.1\n",
    "        else:\n",
    "            lr = self.initial_learning_rate\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "        # Use larger batch size as training progresses for efficiency\n",
    "        batch_size = min(128, 32 + self.steps // 50000)\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            return self.model.fit(\n",
    "                x, y, \n",
    "                batch_size=batch_size, \n",
    "                sample_weight=sample_weight,\n",
    "                epochs=epochs, \n",
    "                verbose=verbose, \n",
    "                callbacks=[self.tensorboard_callback]\n",
    "            )\n",
    "        else:\n",
    "            return self.model.fit(\n",
    "                x, y, \n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs, \n",
    "                verbose=verbose, \n",
    "                callbacks=[self.tensorboard_callback]\n",
    "            )\n",
    "            \n",
    "    def train_on_batch(self, x, y, sample_weight=None):\n",
    "        \"\"\"Efficient batch training with gradient clipping\"\"\"\n",
    "        return self.model.train_on_batch(x, y, sample_weight=sample_weight)\n",
    "        \n",
    "    def predict(self, s, target=False):\n",
    "        \"\"\"Prediction with error handling\"\"\"\n",
    "        try:\n",
    "            if target:\n",
    "                return self.target_model.predict(s, verbose=0)\n",
    "            else:\n",
    "                return self.model.predict(s, verbose=0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction: {e}\")\n",
    "            print(f\"Input shape: {s.shape}\")\n",
    "            # Return zeros if prediction fails\n",
    "            if len(s.shape) == 4:  # Batch of states\n",
    "                return np.zeros((s.shape[0], self.actionCnt))\n",
    "            else:  # Single state\n",
    "                return np.zeros(self.actionCnt)\n",
    "                \n",
    "    def predict_one(self, s, target=False):\n",
    "        \"\"\"Predict Q-values for a single state\"\"\"\n",
    "        # Ensure correct shape: HWC format with batch dimension\n",
    "        if len(s.shape) == 3:  # If no batch dimension\n",
    "            s = np.expand_dims(s, axis=0)  # Add batch dimension\n",
    "        return self.predict(s, target).flatten()\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update target model with current weights\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def save_models(self, path, prefix=\"\"):\n",
    "        \"\"\"Save both models with error handling\"\"\"\n",
    "        try:\n",
    "            self.model.save(f\"{path}/{prefix}model.h5\")\n",
    "            self.target_model.save(f\"{path}/{prefix}target_model.h5\")\n",
    "            print(f\"Models saved to {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving models: {e}\")\n",
    "            return False\n",
    "\n",
    "# Enhanced Memory implementation with fixed-size efficient sampling\n",
    "class Memory:\n",
    "    def __init__(self, capacity, epsilon=0.01, alpha=0.6, beta=0.4, beta_increment=0.001):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.epsilon = epsilon  # small amount to avoid zero priority\n",
    "        self.alpha = alpha      # how much prioritization is used\n",
    "        self.beta = beta        # importance-sampling, increases to 1 over time\n",
    "        self.beta_increment = beta_increment\n",
    "        self.capacity = capacity\n",
    "        self.max_priority = 1.0\n",
    "        \n",
    "    def _getPriority(self, error):\n",
    "        \"\"\"Calculate priority based on TD error\"\"\"\n",
    "        return (np.abs(error) + self.epsilon) ** self.alpha\n",
    "        \n",
    "    def add(self, error, sample):\n",
    "        \"\"\"Add experience to memory with prioritization\"\"\"\n",
    "        # Use max priority for new samples to ensure they get sampled\n",
    "        p = self.max_priority if error is None else self._getPriority(error)\n",
    "        self.tree.add(p, sample)\n",
    "        \n",
    "        # Update max priority\n",
    "        if p > self.max_priority:\n",
    "            self.max_priority = p\n",
    "            \n",
    "    def sample(self, n):\n",
    "        \"\"\"Sample batch with importance sampling weights\"\"\"\n",
    "        # Increase beta over time for annealing\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        \n",
    "        # More efficient batch sampling\n",
    "        indices, priorities, samples = self.tree.get_batch(n)\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        sampling_probabilities = np.array(priorities) / self.tree.total()\n",
    "        weights = (self.capacity * sampling_probabilities) ** (-self.beta)\n",
    "        weights /= weights.max()  # Normalize for stability\n",
    "        \n",
    "        return list(zip(indices, samples)), weights\n",
    "        \n",
    "    def update(self, idx, error):\n",
    "        \"\"\"Update priorities based on new TD errors\"\"\"\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "        \n",
    "        # Update max priority\n",
    "        if p > self.max_priority:\n",
    "            self.max_priority = p\n",
    "            \n",
    "    def size(self):\n",
    "        \"\"\"Get current memory size\"\"\"\n",
    "        return self.tree.n_entries\n",
    "\n",
    "class LearningAgent:\n",
    "    steps = 0\n",
    "    latest_Q = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "    difficulty_level = 5  # Starting difficulty level (1-10)\n",
    "    combo_counter = 0\n",
    "    defensive_stance = False\n",
    "\n",
    "    def __init__(self, learning=False, epsilon=1.0, alpha=0.5):\n",
    "        self.input_handler = InputHandler()\n",
    "        self.learning = learning\n",
    "        # HWC format for input shape\n",
    "        self.inputShape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_STACK)\n",
    "        self.numActions = len(valid_actions)\n",
    "        self.model = Model(self.inputShape, self.numActions, use_dueling=True)\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "        # Efficient data tracking with fixed-size arrays\n",
    "        self.rewards_history = deque(maxlen=10000)\n",
    "        self.q_values_history = deque(maxlen=10000)\n",
    "        self.loss_history = deque(maxlen=10000)\n",
    "        \n",
    "        # Action tracking for diversity\n",
    "        self.prev_actions = deque(maxlen=10)\n",
    "        self.action_repeat_penalty = -0.1\n",
    "        \n",
    "        # Frame skipping counter\n",
    "        self.frame_skip_counter = 0\n",
    "        self.last_action = 0\n",
    "        \n",
    "        # Combo tracking\n",
    "        self.current_combo = 0\n",
    "        self.max_combo = 0\n",
    "        \n",
    "    def observe(self, sample):\n",
    "        \"\"\"Process and store experience in replay memory\"\"\"\n",
    "        s, a, r, s_ = sample\n",
    "        \n",
    "        # Apply data augmentation for better generalization\n",
    "        if random.random() < 0.2:\n",
    "            s = augment_state(s)\n",
    "            if s_ is not None:\n",
    "                s_ = augment_state(s_)\n",
    "        \n",
    "        # Get targets and errors for prioritized replay\n",
    "        x, y, errors = self.get_targets([(0, (s, a, r, s_))])\n",
    "        self.memory.add(errors[0], (s, a, r, s_))\n",
    "\n",
    "        # Update target network periodically\n",
    "        if self.steps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            self.model.update_target_model()\n",
    "            print(f\"Target network updated at step {self.steps}\")\n",
    "\n",
    "        # Update epsilon with decay schedule\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "        \n",
    "        # Track reward for metrics\n",
    "        self.rewards_history.append(r)\n",
    "        \n",
    "        # Log Q values periodically\n",
    "        if self.steps % 100 == 0:\n",
    "            self.q_values_history.append(np.mean(self.model.predict_one(s)))\n",
    "            \n",
    "            # Print progress\n",
    "            avg_reward = np.mean(list(self.rewards_history)[-100:]) if len(self.rewards_history) >= 100 else np.mean(list(self.rewards_history))\n",
    "            print(f\"Step: {self.steps}, Epsilon: {self.epsilon:.4f}, Avg Reward: {avg_reward:.4f}, Latest Q: {self.latest_Q:.4f}\")\n",
    "\n",
    "    def get_targets(self, batch):\n",
    "        \"\"\"Calculate target Q values using Double DQN\"\"\"\n",
    "        no_state = np.zeros(self.inputShape)\n",
    "\n",
    "        states = np.array([o[1][0] for o in batch])\n",
    "        states_ = np.array([(no_state if o[1][3] is None else o[1][3]) for o in batch])\n",
    "\n",
    "        # Predict Q values from current and target networks\n",
    "        p = self.model.predict(states)\n",
    "        p_ = self.model.predict(states_, target=False)  # model predictions for next states\n",
    "        pTarget_ = self.model.predict(states_, target=True)  # target model predictions for next states\n",
    "\n",
    "        x = np.zeros((len(batch), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_STACK))\n",
    "        y = np.zeros((len(batch), self.numActions))\n",
    "        errors = np.zeros(len(batch))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            o = batch[i][1]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i].copy()\n",
    "            oldVal = t[a]\n",
    "            \n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                # Double Deep Q-Learning update\n",
    "                a_ = np.argmax(p_[i])  # Action selection from online network\n",
    "                t[a] = r + GAMMA * pTarget_[i][a_]  # Value from target network\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "            # TD error for prioritized replay\n",
    "            errors[i] = abs(oldVal - t[a])\n",
    "            self.latest_Q = t[a]\n",
    "\n",
    "        return (x, y, errors)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy with diversity bonus\"\"\"\n",
    "        self.state = state\n",
    "        \n",
    "        # Apply frame skipping - repeat last action for efficiency\n",
    "        if self.frame_skip_counter < FRAME_SKIP and self.frame_skip_counter > 0:\n",
    "            self.frame_skip_counter += 1\n",
    "            return self.last_action\n",
    "        else:\n",
    "            self.frame_skip_counter = 1\n",
    "        \n",
    "        # Apply action repeating penalty to encourage diversity\n",
    "        action_penalties = np.zeros(self.numActions)\n",
    "        for prev_action in self.prev_actions:\n",
    "            action_penalties[prev_action] += self.action_repeat_penalty\n",
    "        \n",
    "        action = None\n",
    "\n",
    "        if not self.learning:\n",
    "            action = random.randint(0, len(valid_actions)-1)\n",
    "        else:\n",
    "            # Exploration vs exploitation\n",
    "            if random.uniform(0, 1) < self.epsilon:\n",
    "                action = random.randint(0, len(valid_actions)-1)\n",
    "                print(\"Exploring with random action:\", action)\n",
    "            else:\n",
    "                # Get Q values and apply penalties for repeated actions\n",
    "                q_values = self.model.predict_one(state)\n",
    "                adjusted_q_values = q_values + action_penalties\n",
    "                action = np.argmax(adjusted_q_values)\n",
    "                print(f\"Exploiting with action {action}, Q-value: {q_values[action]:.4f}\")\n",
    "        \n",
    "        # Update previous actions list for action diversity\n",
    "        self.prev_actions.append(action)\n",
    "        self.last_action = action\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def execute_action(self, action):\n",
    "        \"\"\"Execute selected action in game environment\"\"\"\n",
    "        self.input_handler.execute_action(action)\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Experience replay with prioritized sampling\"\"\"\n",
    "        # Wait until we have enough samples\n",
    "        if self.memory.size() < BATCH_SIZE:\n",
    "            return None\n",
    "            \n",
    "        # Sample batch with importance sampling weights\n",
    "        batch, is_weights = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # Get targets and errors for network update\n",
    "        x, y, errors = self.get_targets(batch)\n",
    "        \n",
    "        # Update priorities in memory\n",
    "        for i, (idx, _) in enumerate(batch):\n",
    "            self.memory.update(idx, errors[i])\n",
    "        \n",
    "        # Train the model with importance sampling weights\n",
    "        loss = self.model.train_on_batch(x, y, sample_weight=is_weights)\n",
    "        \n",
    "        # Track loss for metrics\n",
    "        self.loss_history.append(loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def play(self, state):\n",
    "        \"\"\"Used during evaluation/deployment without exploration\"\"\"\n",
    "        self.state = state\n",
    "        # Always choose the best action during play\n",
    "        action = np.argmax(self.model.predict_one(state))\n",
    "        return action\n",
    "        \n",
    "    def adapt_difficulty(self, mean_reward):\n",
    "        \"\"\"Adjust opponent AI difficulty based on agent performance\"\"\"\n",
    "        if mean_reward > 5.0:\n",
    "            # Increase difficulty when agent performs well\n",
    "            self.difficulty_level = min(10, self.difficulty_level + 1)\n",
    "            print(f\"Increasing difficulty to level {self.difficulty_level}\")\n",
    "        elif mean_reward < -5.0:\n",
    "            # Decrease difficulty when agent struggles\n",
    "            self.difficulty_level = max(1, self.difficulty_level - 1)\n",
    "            print(f\"Decreasing difficulty to level {self.difficulty_level}\")\n",
    "        \n",
    "        # Return current difficulty level for game setup\n",
    "        return self.difficulty_level\n",
    "        \n",
    "    def update_combo(self, hit_successful):\n",
    "        \"\"\"Track combo counter for reward shaping\"\"\"\n",
    "        if hit_successful:\n",
    "            self.current_combo += 1\n",
    "            self.max_combo = max(self.max_combo, self.current_combo)\n",
    "        else:\n",
    "            self.current_combo = 0\n",
    "            \n",
    "    def get_combo_bonus(self):\n",
    "        \"\"\"Calculate bonus reward based on current combo\"\"\"\n",
    "        return min(0.5, self.current_combo * 0.1)\n",
    "\n",
    "class Vision:\n",
    "    screen = {'top': 110, 'left': 0, 'width': 1920, 'height': 970}\n",
    "    leftHPCapture = {'top': 110, 'left': 240, 'width': 620, 'height': 20}\n",
    "    rightHPCapture = {'top': 110, 'left': 1043, 'width': 620, 'height': 20}\n",
    "    \n",
    "    positive = 1    # AI hit the opponent\n",
    "    negative = -1   # AI took a hit\n",
    "    \n",
    "    def __init__(self, side):\n",
    "        self.side = side\n",
    "        with mss.mss() as sct:\n",
    "            self.prevLeftHP = self.numpy_img_to_gray(np.array(sct.grab(self.leftHPCapture)))\n",
    "            self.prevRightHP = self.numpy_img_to_gray(np.array(sct.grab(self.rightHPCapture)))\n",
    "        \n",
    "        # For combo tracking\n",
    "        self.combo_counter = 0\n",
    "        self.last_hit_time = 0\n",
    "        self.combo_timeout = 1.0  # seconds between hits to count as combo\n",
    "        \n",
    "        # For defensive move detection\n",
    "        self.defensive_stance = False\n",
    "        self.last_damage_taken = 0\n",
    "        \n",
    "        # For state normalization\n",
    "        self.frames_seen = 0\n",
    "        self.running_mean = 0\n",
    "        self.running_std = 0\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.frame_times = deque(maxlen=100)\n",
    "        self.last_frame_time = time.time()\n",
    "\n",
    "    def numpy_img_to_gray(self, img):\n",
    "        \"\"\"Convert RGB image to grayscale efficiently\"\"\"\n",
    "        return np.dot(img[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "    def get_current_screen(self):\n",
    "        \"\"\"Capture and preprocess current screen\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with mss.mss() as sct:\n",
    "            sct_img = sct.grab(self.screen)\n",
    "            img = Image.frombytes('RGB', sct_img.size, sct_img.rgb)\n",
    "            img = img.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)\n",
    "            currScreen = np.array(img)\n",
    "            \n",
    "        # Preprocess the screen\n",
    "        processed = preprocess_frame(currScreen)\n",
    "        \n",
    "        # Update running statistics for normalization (optional)\n",
    "        self.frames_seen += 1\n",
    "        delta = processed.mean() - self.running_mean\n",
    "        self.running_mean += delta / self.frames_seen\n",
    "        delta2 = processed.mean() - self.running_mean\n",
    "        self.running_std += delta * delta2\n",
    "        \n",
    "        # Track frame processing time\n",
    "        frame_time = time.time() - start_time\n",
    "        self.frame_times.append(frame_time)\n",
    "        \n",
    "        # Print average FPS periodically\n",
    "        if self.frames_seen % 100 == 0:\n",
    "            avg_frame_time = np.mean(self.frame_times)\n",
    "            fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0\n",
    "            print(f\"Average FPS: {fps:.1f}\")\n",
    "            \n",
    "        return processed\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Calculate reward based on game state with improved reward shaping\"\"\"\n",
    "        with mss.mss() as sct:\n",
    "            reward = 0\n",
    "            hit_landed = False\n",
    "            took_damage = False\n",
    "\n",
    "            currLeftHP = np.array(sct.grab(self.leftHPCapture))\n",
    "            currRightHP = np.array(sct.grab(self.rightHPCapture))\n",
    "            \n",
    "            # Convert to gray\n",
    "            currLeftHP = self.numpy_img_to_gray(currLeftHP)\n",
    "            currRightHP = self.numpy_img_to_gray(currRightHP)\n",
    "            \n",
    "            # Get the difference in previous vs current\n",
    "            diffLeftHP = self.prevLeftHP - currLeftHP\n",
    "            diffRightHP = self.prevRightHP - currRightHP\n",
    "            \n",
    "            # Round negative values up to 0\n",
    "            diffLeftHP = diffLeftHP.clip(min=0)\n",
    "            diffRightHP = diffRightHP.clip(min=0)\n",
    "            \n",
    "            # Calculate damage amounts for more granular rewards\n",
    "            left_damage_amount = (diffLeftHP > 125).sum() / 100.0\n",
    "            right_damage_amount = (diffRightHP > 125).sum() / 100.0\n",
    "            \n",
    "            # Basic reward calculation based on side\n",
    "            if left_damage_amount > 0.1:  # Left character took damage\n",
    "                if self.side == 'left':\n",
    "                    reward -= 1 + left_damage_amount  # Penalize based on damage amount\n",
    "                    self.defensive_stance = True\n",
    "                    self.last_damage_taken = time.time()\n",
    "                    self.combo_counter = 0\n",
    "                    took_damage = True\n",
    "                else:\n",
    "                    reward += 1 + left_damage_amount  # Reward based on damage amount\n",
    "                    self.update_combo()\n",
    "                    hit_landed = True\n",
    "                    \n",
    "            if right_damage_amount > 0.1:  # Right character took damage\n",
    "                if self.side == 'right':\n",
    "                    reward -= 1 + right_damage_amount\n",
    "                    self.defensive_stance = True\n",
    "                    self.last_damage_taken = time.time()\n",
    "                    self.combo_counter = 0\n",
    "                    took_damage = True\n",
    "                else:\n",
    "                    reward += 1 + right_damage_amount\n",
    "                    self.update_combo()\n",
    "                    hit_landed = True\n",
    "            \n",
    "            # Add combo bonus\n",
    "            if self.combo_counter > 1:\n",
    "                combo_bonus = min(self.combo_counter * 0.2, 1.5)\n",
    "                reward += combo_bonus\n",
    "                \n",
    "            # Add time penalty (small) to encourage action\n",
    "            time_penalty = -0.005\n",
    "            reward += time_penalty\n",
    "            \n",
    "            # Add defensive bonus if successful defense\n",
    "            if self.defensive_stance and time.time() - self.last_damage_taken > 2.0:\n",
    "                if not took_damage:\n",
    "                    reward += 0.2  # Bonus for successful defense\n",
    "                self.defensive_stance = False\n",
    "                \n",
    "            # Add position reward based on screen analysis (optional)\n",
    "            # This would require additional screen analysis\n",
    "                \n",
    "            # Set previous frame data to current frame data\n",
    "            self.prevLeftHP = currLeftHP\n",
    "            self.prevRightHP = currRightHP\n",
    "\n",
    "        return reward, hit_landed, took_damage\n",
    "    \n",
    "    def update_combo(self):\n",
    "        \"\"\"Update combo counter based on timing between hits\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Check if this hit is part of a combo (within timeout)\n",
    "        if current_time - self.last_hit_time < self.combo_timeout:\n",
    "            self.combo_counter += 1\n",
    "            print(f\"Combo x{self.combo_counter}!\")\n",
    "        else:\n",
    "            # Start new combo\n",
    "            self.combo_counter = 1\n",
    "            \n",
    "        self.last_hit_time = current_time\n",
    "        return self.combo_counter\n",
    "\n",
    "TRIAL = 1\n",
    "TOTAL_TESTS = 1\n",
    "TOTAL_EPISODES = 100\n",
    "EVAL_INTERVAL = 5  # Evaluate every 5 episodes\n",
    "\n",
    "def save_models(agent, curr_test, is_best=False):\n",
    "    \"\"\"Save model with enhanced naming and best model tracking\"\"\"\n",
    "    directory = 'model backup/trial 2'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    # Save regular checkpoint\n",
    "    model_name = f\"/TekkenBotDDQN_{curr_test}.h5\"\n",
    "    target_name = f\"/TekkenBotDDQN_Target_{curr_test}.h5\"\n",
    "    \n",
    "    if is_best:\n",
    "        # Save as best model\n",
    "        model_name = \"/TekkenBotDDQN_best.h5\"\n",
    "        target_name = \"/TekkenBotDDQN_Target_best.h5\"\n",
    "        \n",
    "    # Save both models\n",
    "    agent.model.model.save(directory + model_name, overwrite=True)\n",
    "    agent.model.target_model.save(directory + target_name, overwrite=True)\n",
    "    \n",
    "    print(f\"Models saved: {model_name} and {target_name}\")\n",
    "    \n",
    "    # Save training history\n",
    "    if len(agent.rewards_history) > 0:\n",
    "        np.save(f\"{directory}/rewards_history.npy\", np.array(list(agent.rewards_history)))\n",
    "    if len(agent.q_values_history) > 0:\n",
    "        np.save(f\"{directory}/q_values_history.npy\", np.array(list(agent.q_values_history)))\n",
    "    if len(agent.loss_history) > 0:\n",
    "        np.save(f\"{directory}/loss_history.npy\", np.array(list(agent.loss_history)))\n",
    "        \n",
    "    # Save replay memory samples for later resume\n",
    "    if agent.memory.size() > 1000:\n",
    "        print(\"Saving memory samples...\")\n",
    "        mem_indices, mem_priorities, mem_samples = [], [], []\n",
    "        for _ in range(1000):  # Save a subset of memory\n",
    "            idx, p, sample = agent.memory.tree.get(random.uniform(0, agent.memory.tree.total()))\n",
    "            mem_indices.append(idx)\n",
    "            mem_priorities.append(p)\n",
    "            mem_samples.append(sample)\n",
    "        \n",
    "        np.save(f\"{directory}/memory_samples.npy\", np.array(mem_samples, dtype=object))\n",
    "        print(\"Memory samples saved.\")\n",
    "\n",
    "def evaluate(agent, num_episodes=10):\n",
    "    \"\"\"More comprehensive evaluation function\"\"\"\n",
    "    global best_eval_reward\n",
    "    \n",
    "    agent.input_handler.activate_remap()\n",
    "    vision = Vision('left')\n",
    "    rewards = []\n",
    "    episode_lengths = []\n",
    "    q_values = []\n",
    "    \n",
    "    print(f\"Starting evaluation for {num_episodes} episodes...\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Get initial observation and create initial state\n",
    "        init_frame = vision.get_current_screen()\n",
    "        \n",
    "        # Create state with stacked frames (more efficient with array)\n",
    "        state = np.array([init_frame] * IMAGE_STACK).transpose(1, 2, 0)\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while not done:\n",
    "            # Use deterministic policy (no exploration)\n",
    "            actionIndex = agent.play(state)\n",
    "            agent.execute_action(actionIndex)\n",
    "            \n",
    "            # Capture new observation and reward\n",
    "            new_obs = vision.get_current_screen()\n",
    "            reward, hit, damage = vision.get_reward()\n",
    "            \n",
    "            # Update agent's internal combo counter\n",
    "            if hit:\n",
    "                agent.update_combo(True)\n",
    "            elif damage:\n",
    "                agent.update_combo(False)\n",
    "            \n",
    "            # Track Q-values during evaluation\n",
    "            q_vals = agent.model.predict_one(state)\n",
    "            q_values.append(np.mean(q_vals))\n",
    "            \n",
    "            # Efficient state update\n",
    "            state = update_state(state, new_obs)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # End episode after 60 seconds\n",
    "            if time.time() - start_time > 59:\n",
    "                done = True\n",
    "                \n",
    "        rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        print(f\"Eval Episode {episode+1}: Reward = {total_reward:.2f}, Steps = {steps}, Max Combo: {agent.max_combo}\")\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    mean_length = np.mean(episode_lengths)\n",
    "    std_length = np.std(episode_lengths)\n",
    "    mean_q = np.mean(q_values) if q_values else 0\n",
    "    \n",
    "    print(f'Evaluation Results:')\n",
    "    print(f'Average Reward: {mean_reward:.2f} Â± {std_reward:.2f}')\n",
    "    print(f'Average Episode Length: {mean_length:.2f} Â± {std_length:.2f}')\n",
    "    print(f'Average Q Value: {mean_q:.4f}')\n",
    "    \n",
    "    # Save best model if this is the best performance\n",
    "    if mean_reward > best_eval_reward:\n",
    "        best_eval_reward = mean_reward\n",
    "        save_models(agent, -1, is_best=True)\n",
    "        print(f\"New best model saved with reward: {mean_reward:.2f}\")\n",
    "    \n",
    "    # Return metrics for tracking\n",
    "    return {\n",
    "        'rewards': rewards,\n",
    "        'mean_reward': mean_reward,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'mean_length': mean_length,\n",
    "        'mean_q': mean_q,\n",
    "        'max_combo': agent.max_combo\n",
    "    }\n",
    "\n",
    "def import_model(agent):\n",
    "    \"\"\"Import saved model with better error handling\"\"\"\n",
    "    try:\n",
    "        # Try to load best model first\n",
    "        best_model_path = 'model backup/trial 2/TekkenBotDDQN_best.h5'\n",
    "        best_target_path = 'model backup/trial 2/TekkenBotDDQN_Target_best.h5'\n",
    "        \n",
    "        if os.path.exists(best_model_path) and os.path.exists(best_target_path):\n",
    "            agent.model = Model(agent.inputShape, agent.numActions,\n",
    "                load_model(best_model_path, custom_objects={'huber_loss': huber_loss}),\n",
    "                load_model(best_target_path, custom_objects={'huber_loss': huber_loss}),\n",
    "                use_dueling=True)\n",
    "            print('Best model loaded successfully.')\n",
    "            \n",
    "            # Try to load saved memory samples\n",
    "            memory_path = 'model backup/trial 2/memory_samples.npy'\n",
    "            if os.path.exists(memory_path):\n",
    "                try:\n",
    "                    memory_samples = np.load(memory_path, allow_pickle=True)\n",
    "                    print(f\"Loading {len(memory_samples)} memory samples...\")\n",
    "                    for sample in memory_samples:\n",
    "                        agent.memory.add(None, sample)  # Add with max priority\n",
    "                    print(f\"Memory initialized with {agent.memory.size()} samples.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading memory samples: {e}\")\n",
    "                    \n",
    "            return True\n",
    "            \n",
    "        # Fall back to regular model\n",
    "        model_path = 'model backup/trial 2/TekkenBotDDQN_1.h5'\n",
    "        target_path = 'model backup/trial 2/TekkenBotDDQN_Target_1.h5'\n",
    "        \n",
    "        if os.path.exists(model_path) and os.path.exists(target_path):\n",
    "            agent.model = Model(agent.inputShape, agent.numActions,\n",
    "                load_model(model_path, custom_objects={'huber_loss': huber_loss}),\n",
    "                load_model(target_path, custom_objects={'huber_loss': huber_loss}),\n",
    "                use_dueling=True)\n",
    "            print('Model loaded successfully.')\n",
    "            return True\n",
    "            \n",
    "        print('No existing model found. Starting with fresh model.')\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error loading model:')\n",
    "        print(e)\n",
    "        print('Starting with fresh model.')\n",
    "        return False\n",
    "\n",
    "def play(agent):\n",
    "    \"\"\"Play mode for the trained agent\"\"\"\n",
    "    agent.input_handler.activate_remap()\n",
    "    vision = Vision('left')\n",
    "    \n",
    "    # Initialize with first observation\n",
    "    initial_screen = vision.get_current_screen()\n",
    "    # More efficient state creation with array\n",
    "    state = np.array([initial_screen] * IMAGE_STACK).transpose(1, 2, 0)\n",
    "    \n",
    "    print(\"Starting play mode with trained agent. Press Ctrl+C to stop.\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Choose action using greedy policy\n",
    "            actionIndex = agent.play(state)\n",
    "            agent.execute_action(actionIndex)\n",
    "            \n",
    "            # Get new observation\n",
    "            screenCap = vision.get_current_screen()\n",
    "            reward, hit, damage = vision.get_reward()\n",
    "            \n",
    "            # More efficient state update\n",
    "            state = update_state(state, screenCap)\n",
    "            \n",
    "            # Update combo counter\n",
    "            if hit:\n",
    "                agent.update_combo(True)\n",
    "                print(f\"Hit landed! Combo: {agent.current_combo}\")\n",
    "            elif damage:\n",
    "                agent.update_combo(False)\n",
    "                print(\"Took damage!\")\n",
    "            \n",
    "            # Optional: add short sleep to control action rate\n",
    "            time.sleep(0.05)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Play mode stopped.')\n",
    "\n",
    "def run(agent):\n",
    "    \"\"\"Enhanced training function with improved monitoring and evaluation\"\"\"\n",
    "    agent.input_handler.activate_remap()\n",
    "    vision = Vision('left')\n",
    "    \n",
    "    # Get first observation and create initial state\n",
    "    first_obs = vision.get_current_screen()\n",
    "    # More efficient state creation with array instead of stack\n",
    "    state = np.array([first_obs] * IMAGE_STACK).transpose(1, 2, 0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    episode_start_time = time.time()\n",
    "    global_start_time = time.time()\n",
    "    \n",
    "    # Training statistics\n",
    "    reward_total = 0\n",
    "    episode = 0\n",
    "    true_episode = 0\n",
    "    curr_test = 1\n",
    "    record_size = TOTAL_EPISODES * TOTAL_TESTS + 1\n",
    "    # Tracking more metrics\n",
    "    reward_per_episode = np.zeros((record_size, 6))  # Added column for max combo\n",
    "    i = 0\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    eval_results = []\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting training. Press Ctrl+C to stop.\")\n",
    "        while True:\n",
    "            # Choose action based on current policy\n",
    "            actionIndex = agent.choose_action(state)\n",
    "            agent.execute_action(actionIndex)\n",
    "            \n",
    "            # Get new observation and reward\n",
    "            screenCap = vision.get_current_screen()\n",
    "            reward, hit, damage = vision.get_reward()\n",
    "            \n",
    "            # Update agent's combo counter\n",
    "            if hit:\n",
    "                agent.update_combo(True)\n",
    "            elif damage:\n",
    "                agent.update_combo(False)\n",
    "            \n",
    "            # Efficient state update\n",
    "            next_state = update_state(state.copy(), screenCap)\n",
    "            \n",
    "            # Store experience in replay memory\n",
    "            agent.observe((state, actionIndex, reward, next_state))\n",
    "            \n",
    "            # Perform experience replay periodically\n",
    "            if agent.steps % REPLAY_PERIOD == 0:\n",
    "                loss = agent.replay()\n",
    "                if loss is not None:\n",
    "                    agent.loss_history.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            reward_total += reward\n",
    "            \n",
    "            # End of episode (60 seconds)\n",
    "            if time.time() - episode_start_time > 59:\n",
    "                # Record episode stats\n",
    "                avg_q = np.mean(list(agent.q_values_history)[-100:]) if agent.q_values_history else 0\n",
    "                reward_per_episode[i] = [true_episode, reward_total, agent.steps, avg_q, agent.difficulty_level, agent.max_combo]\n",
    "                \n",
    "                # Calculate training speed\n",
    "                elapsed = time.time() - global_start_time\n",
    "                steps_per_second = agent.steps / elapsed if elapsed > 0 else 0\n",
    "                \n",
    "                print(f\"Episode {true_episode+1} completed. Reward: {reward_total:.2f}, Steps: {agent.steps}, \"\n",
    "                      f\"Epsilon: {agent.epsilon:.4f}, Max Combo: {agent.max_combo}, \"\n",
    "                      f\"Steps/sec: {steps_per_second:.1f}\")\n",
    "                \n",
    "                episode += 1\n",
    "                true_episode += 1\n",
    "                i += 1\n",
    "                episode_start_time = time.time()\n",
    "                \n",
    "                # Evaluate periodically\n",
    "                if episode % EVAL_INTERVAL == 0:\n",
    "                    print(f\"Performing evaluation after episode {true_episode}\")\n",
    "                    eval_result = evaluate(agent, num_episodes=5)\n",
    "                    eval_results.append(eval_result)\n",
    "                    \n",
    "                    # Adjust difficulty based on performance\n",
    "                    agent.adapt_difficulty(eval_result['mean_reward'])\n",
    "                \n",
    "                # Reset for next episode\n",
    "                reward_total = 0\n",
    "                \n",
    "            # End of test segment\n",
    "            if episode >= TOTAL_EPISODES:\n",
    "                if curr_test >= TOTAL_TESTS:\n",
    "                    save_models(agent, curr_test)\n",
    "                    print(\"Training completed. Final model saved.\")\n",
    "                    break\n",
    "                else:\n",
    "                    save_models(agent, curr_test)\n",
    "                    curr_test += 1\n",
    "                    episode = 0\n",
    "                    print(f\"Starting test segment {curr_test}\")\n",
    "                    \n",
    "                    # Evaluate between test segments\n",
    "                    evaluate(agent, num_episodes=10)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user.\")\n",
    "    \n",
    "    finally:\n",
    "        # Save final models and training data\n",
    "        print(\"Saving models and training history...\")\n",
    "        save_models(agent, curr_test)\n",
    "        \n",
    "        # Save episode rewards and evaluation results\n",
    "        np.savetxt('model backup/trial 2/episodesAndRewards.txt', reward_per_episode, fmt='%.4f')\n",
    "        print(\"Episodes and rewards saved to episodesAndRewards.txt\")\n",
    "        \n",
    "        # Save evaluation results\n",
    "        if eval_results:\n",
    "            eval_dir = 'model backup/trial 2/evaluations'\n",
    "            if not os.path.exists(eval_dir):\n",
    "                os.makedirs(eval_dir)\n",
    "            np.save(f\"{eval_dir}/eval_results.npy\", eval_results)\n",
    "            \n",
    "        # Final statistics\n",
    "        elapsed = time.time() - global_start_time\n",
    "        hours = elapsed // 3600\n",
    "        minutes = (elapsed % 3600) // 60\n",
    "        seconds = elapsed % 60\n",
    "        \n",
    "        print(f\"Training complete. Total time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "        print(f\"Total steps: {agent.steps}, Steps per second: {agent.steps / elapsed:.1f}\")\n",
    "        print(f\"Max combo achieved: {agent.max_combo}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Create the learning agent with dueling network and improved hyperparameters\n",
    "        agent = LearningAgent(learning=True, epsilon=MAX_EPSILON, alpha=0.6)\n",
    "        \n",
    "        # Try to load existing model if available\n",
    "        imported = import_model(agent)\n",
    "        \n",
    "        # Ask user whether to train or play\n",
    "        mode = input(\"Enter mode (train/play): \").lower()\n",
    "        \n",
    "        if mode == 'train':\n",
    "            print(\"Starting training mode...\")\n",
    "            run(agent)\n",
    "        elif mode == 'play':\n",
    "            if not imported:\n",
    "                print(\"Warning: No model found for play mode. Training a simple model first...\")\n",
    "                # Do a short training session\n",
    "                run(agent)\n",
    "            print(\"Starting play mode...\")\n",
    "            play(agent)\n",
    "        else:\n",
    "            print(\"Invalid mode. Please enter 'train' or 'play'.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        # Print stack trace for debugging\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        print('Session ended. Thanks for playing!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
