{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b889b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 84, 84, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 21, 21, 32)   8224        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " residual_block (ResidualBlock)  (None, 11, 11, 64)  101056      ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " residual_block_1 (ResidualBloc  (None, 11, 11, 64)  74368       ['residual_block[0][0]']         \n",
      " k)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 7744)         0           ['residual_block_1[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          3965440     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          3965440     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 19)           9747        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 1)           0           ['dense_3[0][0]']                \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            513         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 19)           0           ['dense_3[0][0]',                \n",
      "                                                                  'tf.math.reduce_mean[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 19)          0           ['dense_1[0][0]',                \n",
      " da)                                                              'tf.math.subtract[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,124,788\n",
      "Trainable params: 8,124,276\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 84, 84, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 21, 21, 32)   8224        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " residual_block_2 (ResidualBloc  (None, 11, 11, 64)  101056      ['conv2d_6[0][0]']               \n",
      " k)                                                                                               \n",
      "                                                                                                  \n",
      " residual_block_3 (ResidualBloc  (None, 11, 11, 64)  74368       ['residual_block_2[0][0]']       \n",
      " k)                                                                                               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 7744)         0           ['residual_block_3[0][0]']       \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 512)          3965440     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 512)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 512)          3965440     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 19)           9747        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 512)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  (None, 1)           0           ['dense_7[0][0]']                \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            513         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLambda  (None, 19)          0           ['dense_7[0][0]',                \n",
      " )                                                                'tf.math.reduce_mean_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 19)          0           ['dense_5[0][0]',                \n",
      " mbda)                                                            'tf.math.subtract_1[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,124,788\n",
      "Trainable params: 8,124,276\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "Model loaded successfully.\n",
      "Starting training mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dawoo\\AppData\\Local\\Temp\\ipykernel_7616\\2402876234.py:126: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-darkgrid')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressing key: 162\n",
      "Pressing key: 18\n",
      "Releasing key: 162\n",
      "Releasing key: 18\n",
      "Starting training. Press Ctrl+C to stop.\n",
      "Exploring with random action: 5\n",
      "Executing action: 0\n",
      "Pressing key: 88\n",
      "Invalid action structure: 0\n",
      "Releasing key: 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dawoo\\AppData\\Local\\Temp\\ipykernel_7616\\2402876234.py:1435: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  img = img.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target network updated at step 0\n",
      "Executing action: 0\n",
      "Pressing key: 88\n",
      "Invalid action structure: 0\n",
      "Releasing key: 88\n",
      "Exploring with random action: 12\n",
      "Executing action: (65, 87)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Pressing key: 87\n",
      "Releasing key: 87\n",
      "Releasing key: 88\n",
      "Executing action: (65, 87)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Pressing key: 87\n",
      "Releasing key: 87\n",
      "Releasing key: 88\n",
      "Exploring with random action: 14\n",
      "Executing action: (0, (69, 82))\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 69\n",
      "Pressing key: 82\n",
      "Releasing key: 69\n",
      "Releasing key: 82\n",
      "Releasing key: 88\n",
      "Executing action: (0, (69, 82))\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 69\n",
      "Pressing key: 82\n",
      "Releasing key: 69\n",
      "Releasing key: 82\n",
      "Releasing key: 88\n",
      "Exploring with random action: 2\n",
      "Executing action: 84\n",
      "Pressing key: 88\n",
      "Pressing key: 84\n",
      "Releasing key: 84\n",
      "Releasing key: 88\n",
      "Executing action: 84\n",
      "Pressing key: 88\n",
      "Pressing key: 84\n",
      "Releasing key: 84\n",
      "Releasing key: 88\n",
      "Exploring with random action: 8\n",
      "Executing action: 65\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Releasing key: 88\n",
      "Executing action: 65\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Releasing key: 88\n",
      "Exploring with random action: 16\n",
      "Executing action: (69, 89)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 69\n",
      "Releasing key: 69\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Releasing key: 88\n",
      "Executing action: (69, 89)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 69\n",
      "Releasing key: 69\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Releasing key: 88\n",
      "Exploring with random action: 5\n",
      "Executing action: 0\n",
      "Pressing key: 88\n",
      "Invalid action structure: 0\n",
      "Releasing key: 88\n",
      "Executing action: 0\n",
      "Pressing key: 88\n",
      "Invalid action structure: 0\n",
      "Releasing key: 88\n",
      "Exploring with random action: 8\n",
      "Executing action: 65\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Releasing key: 88\n",
      "Executing action: 65\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Releasing key: 88\n",
      "Exploring with random action: 8\n",
      "Executing action: 65\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Releasing key: 88\n",
      "Executing action: 65\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Releasing key: 88\n",
      "Exploring with random action: 11\n",
      "Executing action: (68, 83)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 68\n",
      "Releasing key: 68\n",
      "Pressing key: 83\n",
      "Releasing key: 83\n",
      "Releasing key: 88\n",
      "Executing action: (68, 83)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 68\n",
      "Releasing key: 68\n",
      "Pressing key: 83\n",
      "Releasing key: 83\n",
      "Releasing key: 88\n",
      "Exploring with random action: 7\n",
      "Executing action: 83\n",
      "Pressing key: 88\n",
      "Pressing key: 83\n",
      "Releasing key: 83\n",
      "Releasing key: 88\n",
      "Executing action: 83\n",
      "Pressing key: 88\n",
      "Pressing key: 83\n",
      "Releasing key: 83\n",
      "Releasing key: 88\n",
      "Exploring with random action: 8\n",
      "Executing action: 65\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Releasing key: 88\n",
      "Executing action: 65\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Releasing key: 88\n",
      "Exploring with random action: 16\n",
      "Executing action: (69, 89)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 69\n",
      "Releasing key: 69\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Releasing key: 88\n",
      "Executing action: (69, 89)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 69\n",
      "Releasing key: 69\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Releasing key: 88\n",
      "Exploring with random action: 12\n",
      "Executing action: (65, 87)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Pressing key: 87\n",
      "Releasing key: 87\n",
      "Releasing key: 88\n",
      "Executing action: (65, 87)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Pressing key: 87\n",
      "Releasing key: 87\n",
      "Releasing key: 88\n",
      "Exploring with random action: 5\n",
      "Executing action: 0\n",
      "Pressing key: 88\n",
      "Invalid action structure: 0\n",
      "Releasing key: 88\n",
      "Executing action: 0\n",
      "Pressing key: 88\n",
      "Invalid action structure: 0\n",
      "Releasing key: 88\n",
      "Exploring with random action: 12\n",
      "Executing action: (65, 87)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Pressing key: 87\n",
      "Releasing key: 87\n",
      "Releasing key: 88\n",
      "Executing action: (65, 87)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 65\n",
      "Releasing key: 65\n",
      "Pressing key: 87\n",
      "Releasing key: 87\n",
      "Releasing key: 88\n",
      "Exploring with random action: 1\n",
      "Executing action: 89\n",
      "Pressing key: 88\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Releasing key: 88\n",
      "Executing action: 89\n",
      "Pressing key: 88\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Releasing key: 88\n",
      "Exploring with random action: 15\n",
      "Executing action: (89, 89, 89, 89)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Releasing key: 88\n",
      "Executing action: (89, 89, 89, 89)\n",
      "Pressing key: 88\n",
      "Pressing key: 88\n",
      "Pressing key: 89\n",
      "Releasing key: 89\n",
      "Pressing key: 89\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import cv2\n",
    "import mss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential, Model, load_model,model_from_json\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Input, BatchNormalization, Dropout, Add\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import sys\n",
    "import ctypes\n",
    "from ctypes import wintypes, c_int, byref\n",
    "import time, random\n",
    "from collections import deque, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "user32 = ctypes.WinDLL('user32', use_last_error=True)\n",
    "\n",
    "INPUT_MOUSE    = 0\n",
    "INPUT_KEYBOARD = 1\n",
    "INPUT_HARDWARE = 2\n",
    "\n",
    "KEYEVENTF_EXTENDEDKEY = 0x0001\n",
    "KEYEVENTF_KEYUP       = 0x0002\n",
    "KEYEVENTF_UNICODE     = 0x0004\n",
    "KEYEVENTF_SCANCODE    = 0x0008\n",
    "\n",
    "MAPVK_VK_TO_VSC = 0\n",
    "\n",
    "# msdn.microsoft.com/en-us/library/dd375731\n",
    "VK_LMENU    = 0x12 # Left Alt key 18\n",
    "VK_LCONTROL = 0xA2 # Left Ctrl key 162\n",
    "\n",
    "# Playstation valid buttons\n",
    "DPAD_UP     = 0x57 # W key 87\n",
    "DPAD_DOWN   = 0x53 # S key 83\n",
    "DPAD_LEFT   = 0x41 # A key 65\n",
    "DPAD_RIGHT  = 0x44 # D key 68\n",
    "CROSS       = 0x45 # E key 69\n",
    "SQUARE      = 0x52 # R key 82\n",
    "CIRCLE      = 0x54 # T key 84\n",
    "TRIANGLE    = 0x59 # Y key 89\n",
    "R1          = 0x51 # Q key 81\n",
    "TOUCHPAD    = 0x58 # X l1 key 88\n",
    "\n",
    "# Diagonal directional buttons with arbitrary values\n",
    "DIAG_DOWN_LEFT = (DPAD_LEFT, DPAD_DOWN)\n",
    "DIAG_DOWN_RIGHT = (DPAD_RIGHT, DPAD_DOWN)\n",
    "DIAG_UP_LEFT = (DPAD_LEFT, DPAD_UP)\n",
    "DIAG_UP_RIGHT = (DPAD_RIGHT, DPAD_UP)\n",
    "\n",
    "# Multi button attacks with arbitrary values\n",
    "CROSS_SQUARE = (0,(CROSS, SQUARE))\n",
    "CROSS_CIRCLE = (TRIANGLE,TRIANGLE,TRIANGLE,TRIANGLE)\n",
    "SQUARE_TRIANGLE = (CROSS,TRIANGLE)\n",
    "TRIANGLE_CIRCLE = (0,(TRIANGLE, CIRCLE))\n",
    "\n",
    "ATTACK01 = (SQUARE,TRIANGLE,DPAD_RIGHT,DPAD_RIGHT,TRIANGLE,DPAD_RIGHT,CIRCLE)\n",
    "\n",
    "delay = ['hold','tap']\n",
    "# available inputs by type\n",
    "direction = [0, DPAD_UP, DPAD_DOWN, DPAD_LEFT, DPAD_RIGHT, DIAG_DOWN_LEFT, DIAG_DOWN_RIGHT, DIAG_UP_LEFT, DIAG_UP_RIGHT]\n",
    "attack = [ATTACK01,TRIANGLE, CIRCLE, CROSS, SQUARE,0, DPAD_UP, DPAD_DOWN, DPAD_LEFT, DPAD_RIGHT, DIAG_DOWN_LEFT, DIAG_DOWN_RIGHT, DIAG_UP_LEFT, DIAG_UP_RIGHT,\n",
    "           CROSS_SQUARE, CROSS_CIRCLE, SQUARE_TRIANGLE, TRIANGLE_CIRCLE,R1,]\n",
    "rage = [R1]\n",
    "\n",
    "# all valid actions\n",
    "valid_actions = attack\n",
    "# valid_actions = [(x,y) for x in direction for y in attack]\n",
    "\n",
    "wintypes.ULONG_PTR = wintypes.WPARAM\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Class for monitoring and visualizing the performance of the Tekken Bot\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir='performance_data'):\n",
    "        \"\"\"Initialize the performance monitor\"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            os.makedirs(os.path.join(save_dir, 'plots'))\n",
    "            os.makedirs(os.path.join(save_dir, 'data'))\n",
    "        \n",
    "        # Initialize metrics storage\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.avg_q_values = []\n",
    "        self.losses = []\n",
    "        self.epsilons = []\n",
    "        self.max_combos = []\n",
    "        self.difficulty_levels = []\n",
    "        self.steps_history = []\n",
    "        self.action_distribution = Counter()\n",
    "        self.hit_rates = []\n",
    "        self.damage_taken_rates = []\n",
    "        self.training_times = []\n",
    "        \n",
    "        # Training progress tracking\n",
    "        self.episodes_completed = 0\n",
    "        self.total_steps = 0\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        self.eval_rewards = []\n",
    "        self.eval_steps = []\n",
    "        self.eval_q_values = []\n",
    "        self.eval_max_combos = []\n",
    "        self.eval_episodes = []\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.fps_history = []\n",
    "        self.training_speed_history = []  # steps per second\n",
    "        self.memory_usage = []\n",
    "        \n",
    "        # Initialize figure for real-time plotting\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "        plt.ion()  # Turn on interactive mode\n",
    "        self.fig_size = (12, 10)\n",
    "        \n",
    "    def update(self, episode, reward, steps, avg_q, epsilon, loss, max_combo, difficulty_level, hits=0, damage_taken=0, actions=None):\n",
    "        \"\"\"Update metrics after each episode\"\"\"\n",
    "        self.episodes_completed = episode\n",
    "        self.total_steps = steps\n",
    "        \n",
    "        # Store episode metrics\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_lengths.append(steps - sum(self.episode_lengths) if self.episode_lengths else steps)\n",
    "        self.avg_q_values.append(avg_q)\n",
    "        self.epsilons.append(epsilon)\n",
    "        self.max_combos.append(max_combo)\n",
    "        self.difficulty_levels.append(difficulty_level)\n",
    "        self.steps_history.append(steps)\n",
    "        \n",
    "        if loss is not None:\n",
    "            self.losses.append(loss)\n",
    "        \n",
    "        # Store action distribution\n",
    "        if actions is not None:\n",
    "            self.action_distribution.update(actions)\n",
    "            \n",
    "        # Store hit and damage metrics\n",
    "        self.hit_rates.append(hits)\n",
    "        self.damage_taken_rates.append(damage_taken)\n",
    "        \n",
    "        # Store training time\n",
    "        current_time = datetime.now()\n",
    "        elapsed_time = (current_time - self.start_time).total_seconds()\n",
    "        self.training_times.append(elapsed_time)\n",
    "        \n",
    "        # Calculate training speed (steps per second)\n",
    "        steps_per_second = steps / elapsed_time if elapsed_time > 0 else 0\n",
    "        self.training_speed_history.append(steps_per_second)\n",
    "        \n",
    "        # Save data periodically\n",
    "        if episode % 5 == 0:\n",
    "            self.save_data()\n",
    "        \n",
    "    def update_eval(self, episode, eval_results):\n",
    "        \"\"\"Update evaluation metrics\"\"\"\n",
    "        self.eval_episodes.append(episode)\n",
    "        self.eval_rewards.append(eval_results['mean_reward'])\n",
    "        self.eval_steps.append(eval_results['mean_length'])\n",
    "        self.eval_q_values.append(eval_results['mean_q'])\n",
    "        self.eval_max_combos.append(eval_results['max_combo'])\n",
    "        \n",
    "        # Save evaluation data\n",
    "        self.save_data()\n",
    "        \n",
    "    def update_fps(self, fps):\n",
    "        \"\"\"Update FPS metrics\"\"\"\n",
    "        self.fps_history.append(fps)\n",
    "        \n",
    "    def plot_and_save(self):\n",
    "        \"\"\"Create and save comprehensive performance plots\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "        # Create main dashboard\n",
    "        self.create_performance_dashboard(timestamp)\n",
    "        \n",
    "        # Create detailed individual plots\n",
    "        self.plot_rewards(timestamp)\n",
    "        self.plot_q_values(timestamp)\n",
    "        self.plot_loss(timestamp)\n",
    "        self.plot_exploration(timestamp)\n",
    "        self.plot_combos(timestamp)\n",
    "        self.plot_action_distribution(timestamp)\n",
    "        self.plot_training_speed(timestamp)\n",
    "        self.plot_evaluation_metrics(timestamp)\n",
    "        \n",
    "        print(f\"Performance plots saved to {self.save_dir}/plots/\")\n",
    "        \n",
    "    def create_performance_dashboard(self, timestamp):\n",
    "        \"\"\"Create a comprehensive dashboard with key metrics\"\"\"\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Episode rewards\n",
    "        plt.subplot(3, 2, 1)\n",
    "        plt.plot(self.episode_rewards, label='Episode Reward')\n",
    "        if self.eval_episodes:\n",
    "            eval_x = self.eval_episodes\n",
    "            plt.scatter(eval_x, self.eval_rewards, color='red', label='Evaluation')\n",
    "        plt.title('Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Q-values\n",
    "        plt.subplot(3, 2, 2)\n",
    "        plt.plot(self.avg_q_values, label='Avg Q-Value')\n",
    "        plt.title('Average Q-Values')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Q-Value')\n",
    "        \n",
    "        # Loss\n",
    "        if self.losses:\n",
    "            plt.subplot(3, 2, 3)\n",
    "            plt.plot(self.losses)\n",
    "            plt.title('Loss')\n",
    "            plt.xlabel('Training Step (sampled)')\n",
    "            plt.ylabel('Loss')\n",
    "        \n",
    "        # Epsilon\n",
    "        plt.subplot(3, 2, 4)\n",
    "        plt.plot(self.epsilons)\n",
    "        plt.title('Exploration Rate (Epsilon)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Epsilon')\n",
    "        \n",
    "        # Max combos\n",
    "        plt.subplot(3, 2, 5)\n",
    "        plt.plot(self.max_combos, label='Max Combo')\n",
    "        plt.title('Max Combo per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Max Combo')\n",
    "        \n",
    "        # Training speed\n",
    "        plt.subplot(3, 2, 6)\n",
    "        plt.plot(self.training_speed_history)\n",
    "        plt.title('Training Speed')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps per Second')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.save_dir}/plots/dashboard_{timestamp}.png\", dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_rewards(self, timestamp):\n",
    "        \"\"\"Create and save reward plots\"\"\"\n",
    "        plt.figure(figsize=self.fig_size)\n",
    "        \n",
    "        # Raw rewards\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(self.episode_rewards)\n",
    "        plt.title('Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        \n",
    "        # Moving average\n",
    "        plt.subplot(2, 1, 2)\n",
    "        window_size = min(10, len(self.episode_rewards))\n",
    "        if window_size > 0:\n",
    "            moving_avg = pd.Series(self.episode_rewards).rolling(window=window_size).mean()\n",
    "            plt.plot(moving_avg)\n",
    "            plt.title(f'Moving Average Reward (Window={window_size})')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Average Reward')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.save_dir}/plots/rewards_{timestamp}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_q_values(self, timestamp):\n",
    "        \"\"\"Create and save Q-value plots\"\"\"\n",
    "        plt.figure(figsize=self.fig_size)\n",
    "        \n",
    "        plt.plot(self.avg_q_values)\n",
    "        plt.title('Average Q-Values')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Q-Value')\n",
    "        \n",
    "        plt.savefig(f\"{self.save_dir}/plots/q_values_{timestamp}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_loss(self, timestamp):\n",
    "        \"\"\"Create and save loss plots\"\"\"\n",
    "        if not self.losses:\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=self.fig_size)\n",
    "        \n",
    "        plt.plot(self.losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Training Step (sampled)')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        plt.savefig(f\"{self.save_dir}/plots/loss_{timestamp}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_exploration(self, timestamp):\n",
    "        \"\"\"Create and save exploration rate plots\"\"\"\n",
    "        plt.figure(figsize=self.fig_size)\n",
    "        \n",
    "        plt.plot(self.epsilons)\n",
    "        plt.title('Exploration Rate (Epsilon)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Epsilon')\n",
    "        \n",
    "        plt.savefig(f\"{self.save_dir}/plots/exploration_{timestamp}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_combos(self, timestamp):\n",
    "        \"\"\"Create and save combo metrics plots\"\"\"\n",
    "        plt.figure(figsize=self.fig_size)\n",
    "        \n",
    "        plt.plot(self.max_combos)\n",
    "        plt.title('Max Combo per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Max Combo')\n",
    "        \n",
    "        plt.savefig(f\"{self.save_dir}/plots/combos_{timestamp}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_action_distribution(self, timestamp):\n",
    "        \"\"\"Create and save action distribution plots\"\"\"\n",
    "        if not self.action_distribution:\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=self.fig_size)\n",
    "        \n",
    "        actions = list(self.action_distribution.keys())\n",
    "        counts = list(self.action_distribution.values())\n",
    "        \n",
    "        # Sort by action index\n",
    "        sorted_indices = np.argsort(actions)\n",
    "        actions = [actions[i] for i in sorted_indices]\n",
    "        counts = [counts[i] for i in sorted_indices]\n",
    "        \n",
    "        plt.bar(actions, counts)\n",
    "        plt.title('Action Distribution')\n",
    "        plt.xlabel('Action Index')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        plt.savefig(f\"{self.save_dir}/plots/action_distribution_{timestamp}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_training_speed(self, timestamp):\n",
    "        \"\"\"Create and save training speed plots\"\"\"\n",
    "        plt.figure(figsize=self.fig_size)\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(self.training_speed_history)\n",
    "        plt.title('Training Speed')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps per Second')\n",
    "        \n",
    "        if self.fps_history:\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(self.fps_history)\n",
    "            plt.title('Frames Per Second')\n",
    "            plt.xlabel('Sample')\n",
    "            plt.ylabel('FPS')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.save_dir}/plots/training_speed_{timestamp}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_evaluation_metrics(self, timestamp):\n",
    "        \"\"\"Create and save evaluation metrics plots\"\"\"\n",
    "        if not self.eval_episodes:\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Evaluation rewards\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(self.eval_episodes, self.eval_rewards, marker='o')\n",
    "        plt.title('Evaluation Rewards')\n",
    "        plt.xlabel('Training Episode')\n",
    "        plt.ylabel('Mean Reward')\n",
    "        \n",
    "        # Evaluation Q-values\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(self.eval_episodes, self.eval_q_values, marker='o')\n",
    "        plt.title('Evaluation Q-Values')\n",
    "        plt.xlabel('Training Episode')\n",
    "        plt.ylabel('Mean Q-Value')\n",
    "        \n",
    "        # Evaluation episode lengths\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(self.eval_episodes, self.eval_steps, marker='o')\n",
    "        plt.title('Evaluation Episode Lengths')\n",
    "        plt.xlabel('Training Episode')\n",
    "        plt.ylabel('Mean Steps')\n",
    "        \n",
    "        # Evaluation max combos\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(self.eval_episodes, self.eval_max_combos, marker='o')\n",
    "        plt.title('Evaluation Max Combos')\n",
    "        plt.xlabel('Training Episode')\n",
    "        plt.ylabel('Max Combo')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.save_dir}/plots/evaluation_{timestamp}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    def save_data(self):\n",
    "        \"\"\"Save metrics to CSV files\"\"\"\n",
    "        # Save training metrics\n",
    "        training_data = {\n",
    "            'episode': list(range(len(self.episode_rewards))),\n",
    "            'reward': self.episode_rewards,\n",
    "            'episode_length': self.episode_lengths,\n",
    "            'q_value': self.avg_q_values,\n",
    "            'epsilon': self.epsilons,\n",
    "            'max_combo': self.max_combos,\n",
    "            'difficulty': self.difficulty_levels,\n",
    "            'total_steps': self.steps_history,\n",
    "            'training_speed': self.training_speed_history,\n",
    "            'training_time': self.training_times\n",
    "        }\n",
    "        \n",
    "        # Create DataFrame and save\n",
    "        df_training = pd.DataFrame(training_data)\n",
    "        df_training.to_csv(f\"{self.save_dir}/data/training_metrics.csv\", index=False)\n",
    "        \n",
    "        # Save evaluation metrics if available\n",
    "        if self.eval_episodes:\n",
    "            eval_data = {\n",
    "                'training_episode': self.eval_episodes,\n",
    "                'eval_reward': self.eval_rewards,\n",
    "                'eval_steps': self.eval_steps,\n",
    "                'eval_q_value': self.eval_q_values,\n",
    "                'eval_max_combo': self.eval_max_combos\n",
    "            }\n",
    "            \n",
    "            df_eval = pd.DataFrame(eval_data)\n",
    "            df_eval.to_csv(f\"{self.save_dir}/data/evaluation_metrics.csv\", index=False)\n",
    "            \n",
    "        # Save action distribution\n",
    "        if self.action_distribution:\n",
    "            action_data = {'action': list(self.action_distribution.keys()),\n",
    "                          'count': list(self.action_distribution.values())}\n",
    "            df_actions = pd.DataFrame(action_data)\n",
    "            df_actions.to_csv(f\"{self.save_dir}/data/action_distribution.csv\", index=False)\n",
    "            \n",
    "        print(f\"Metrics data saved to {self.save_dir}/data/\")\n",
    "        \n",
    "    def get_summary_stats(self):\n",
    "        \"\"\"Get summary statistics of the training\"\"\"\n",
    "        if not self.episode_rewards:\n",
    "            return \"No training data available.\"\n",
    "            \n",
    "        summary = {\n",
    "            'episodes_completed': self.episodes_completed,\n",
    "            'total_steps': self.total_steps,\n",
    "            'avg_reward': np.mean(self.episode_rewards[-10:]) if len(self.episode_rewards) >= 10 else np.mean(self.episode_rewards),\n",
    "            'max_reward': max(self.episode_rewards) if self.episode_rewards else 0,\n",
    "            'avg_q_value': np.mean(self.avg_q_values[-10:]) if len(self.avg_q_values) >= 10 else np.mean(self.avg_q_values),\n",
    "            'current_epsilon': self.epsilons[-1] if self.epsilons else 0,\n",
    "            'max_combo_achieved': max(self.max_combos) if self.max_combos else 0,\n",
    "            'training_time': self.training_times[-1] if self.training_times else 0,\n",
    "            'fps': np.mean(self.fps_history[-100:]) if len(self.fps_history) >= 100 else np.mean(self.fps_history) if self.fps_history else 0,\n",
    "            'steps_per_second': self.training_speed_history[-1] if self.training_speed_history else 0\n",
    "        }\n",
    "        \n",
    "        # Format training time\n",
    "        hours, remainder = divmod(summary['training_time'], 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        summary['training_time_formatted'] = f\"{int(hours)}h {int(minutes)}m {int(seconds)}s\"\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print a formatted summary of the training progress\"\"\"\n",
    "        stats = self.get_summary_stats()\n",
    "        \n",
    "        if isinstance(stats, str):\n",
    "            print(stats)\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRAINING SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Episodes Completed: {stats['episodes_completed']}\")\n",
    "        print(f\"Total Steps: {stats['total_steps']}\")\n",
    "        print(f\"Recent Average Reward: {stats['avg_reward']:.2f}\")\n",
    "        print(f\"Max Reward Achieved: {stats['max_reward']:.2f}\")\n",
    "        print(f\"Current Average Q-Value: {stats['avg_q_value']:.4f}\")\n",
    "        print(f\"Current Exploration Rate: {stats['current_epsilon']:.4f}\")\n",
    "        print(f\"Max Combo Achieved: {stats['max_combo_achieved']}\")\n",
    "        print(f\"Training Speed: {stats['steps_per_second']:.2f} steps/sec\")\n",
    "        print(f\"Average FPS: {stats['fps']:.1f}\")\n",
    "        print(f\"Total Training Time: {stats['training_time_formatted']}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "class MOUSEINPUT(ctypes.Structure):\n",
    "    _fields_ = ((\"dx\",          wintypes.LONG),\n",
    "                (\"dy\",          wintypes.LONG),\n",
    "                (\"mouseData\",   wintypes.DWORD),\n",
    "                (\"dwFlags\",     wintypes.DWORD),\n",
    "                (\"time\",        wintypes.DWORD),\n",
    "                (\"dwExtraInfo\", wintypes.ULONG_PTR))\n",
    "\n",
    "class KEYBDINPUT(ctypes.Structure):\n",
    "    _fields_ = ((\"wVk\",         wintypes.WORD),\n",
    "                (\"wScan\",       wintypes.WORD),\n",
    "                (\"dwFlags\",     wintypes.DWORD),\n",
    "                (\"time\",        wintypes.DWORD),\n",
    "                (\"dwExtraInfo\", wintypes.ULONG_PTR))\n",
    "\n",
    "    def __init__(self, *args, **kwds):\n",
    "        super(KEYBDINPUT, self).__init__(*args, **kwds)\n",
    "        # some programs use the scan code even if KEYEVENTF_SCANCODE\n",
    "        # isn't set in dwFflags, so attempt to map the correct code.\n",
    "        if not self.dwFlags & KEYEVENTF_UNICODE:\n",
    "            self.wScan = user32.MapVirtualKeyExW(self.wVk,\n",
    "                                                 MAPVK_VK_TO_VSC, 0)\n",
    "class HARDWAREINPUT(ctypes.Structure):\n",
    "    _fields_ = ((\"uMsg\",    wintypes.DWORD),\n",
    "                (\"wParamL\", wintypes.WORD),\n",
    "                (\"wParamH\", wintypes.WORD))\n",
    "\n",
    "class INPUT(ctypes.Structure):\n",
    "    class _INPUT(ctypes.Union):\n",
    "        _fields_ = ((\"ki\", KEYBDINPUT),\n",
    "                    (\"mi\", MOUSEINPUT),\n",
    "                    (\"hi\", HARDWAREINPUT))\n",
    "    _anonymous_ = (\"_input\",)\n",
    "    _fields_ = ((\"type\",   wintypes.DWORD),\n",
    "                (\"_input\", _INPUT))\n",
    "\n",
    "def _check_count(result, func, args):\n",
    "    if result == 0:\n",
    "        raise ctypes.WinError(ctypes.get_last_error())\n",
    "    return args\n",
    "\n",
    "class InputHandler:\n",
    "    LPINPUT = ctypes.POINTER(INPUT)\n",
    "    user32.SendInput.errcheck = _check_count\n",
    "    user32.SendInput.argtypes = (wintypes.UINT, # nInputs\n",
    "                                 LPINPUT,       # pInputs\n",
    "                                 ctypes.c_int)  # cbSize\n",
    "\n",
    "    def __init__(self):\n",
    "        self.PS4RemotePlayHWND = 0\n",
    "        self.PS4RemotePlayPID = 0\n",
    "        # Create a command buffer to optimize action execution\n",
    "        self.command_buffer = []\n",
    "        \n",
    "    def queue_command(self, key, action_type, delay=0):\n",
    "        \"\"\"Queue a command to execute later\"\"\"\n",
    "        self.command_buffer.append((key, action_type, delay))\n",
    "        \n",
    "    def execute_command_buffer(self):\n",
    "        \"\"\"Execute all queued commands in sequence\"\"\"\n",
    "        for key, action_type, delay in self.command_buffer:\n",
    "            if action_type == 'press':\n",
    "                self.press_key(key)\n",
    "            elif action_type == 'release':\n",
    "                self.release_key(key)\n",
    "            \n",
    "            if delay > 0:\n",
    "                time.sleep(delay)\n",
    "                \n",
    "        # Clear buffer after execution\n",
    "        self.command_buffer = []\n",
    "\n",
    "    def get_actions(self, amount):\n",
    "        actions = []\n",
    "        actions.append([])\n",
    "        action = 0\n",
    "        for i in range(0,amount):\n",
    "            temp = random.randint(0,1)\n",
    "            if temp == 0:\n",
    "                # select something random from the direction arroy\n",
    "                action = direction[random.randint(0,len(direction)-1)]\n",
    "            else:\n",
    "                # select something random from the attack array\n",
    "                action = attack[random.randint(0,len(direction)-1)]\n",
    "            # Only add the input if it is not 0. 0 Is the same as nothing.\n",
    "            if action != 0:\n",
    "                actions.append(action)\n",
    "        # Get the delay time for pressing these keys\n",
    "        delayVal = delay[random.randint(0,1)]\n",
    "        if delayVal == 'hold':\n",
    "            # can't use i as the index because I am only adding non 0 inputs\n",
    "            actions[0].append(random.uniform(0.04, 0.06))\n",
    "        else:\n",
    "            # can't use i as the index because I am only adding non 0 inputs\n",
    "            actions[0].append(random.uniform(0.02, 0.04))\n",
    "        return actions\n",
    "\n",
    "    def get_action(self, index):\n",
    "        print('Valid Actions:', valid_actions[index])\n",
    "        return valid_actions[index]\n",
    "\n",
    "    def get_remote_play_pid(self):\n",
    "        # register winapi functions\n",
    "        EnumWindows = ctypes.windll.user32.EnumWindows\n",
    "        EnumWindowsProc = ctypes.WINFUNCTYPE(ctypes.c_bool, ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int))\n",
    "        GetWindowText = ctypes.windll.user32.GetWindowTextW\n",
    "        GetWindowTextLength = ctypes.windll.user32.GetWindowTextLengthW\n",
    "        IsWindowVisible = ctypes.windll.user32.IsWindowVisible\n",
    "        GetWindowThreadProcessId = ctypes.windll.user32.GetWindowThreadProcessId\n",
    "\n",
    "        def foreach_window(self, hwnd, lParam):\n",
    "            # window must be visible\n",
    "            if IsWindowVisible(hwnd):\n",
    "                length = GetWindowTextLength(hwnd)\n",
    "                buff = ctypes.create_unicode_buffer(length + 1)\n",
    "                GetWindowText(hwnd, buff, length + 1)\n",
    "                try:\n",
    "                    windowtitle = buff.value\n",
    "                    if \"REPL4Y\" in windowtitle:\n",
    "                        # get the processid from the hwnd\n",
    "                        # declaring this as global means refer to the global version\n",
    "                        processID = c_int()\n",
    "                        threadID = GetWindowThreadProcessId(hwnd, byref(processID))\n",
    "                        # found the process ID\n",
    "                        self.PS4RemotePlayPID = processID\n",
    "                        self.PS4RemotePlayHWND = hwnd\n",
    "                        return True\n",
    "                except:\n",
    "                    print(\"Unexpected error:\"+sys.exc_info()[0])\n",
    "                    pass;\n",
    "            return True\n",
    "        EnumWindows(EnumWindowsProc(foreach_window(self)), 0)\n",
    "\n",
    "    def press_key(self, hexKeyCode):\n",
    "        print(f\"Pressing key: {hexKeyCode}\")\n",
    "        x = INPUT(type=INPUT_KEYBOARD,\n",
    "                  ki=KEYBDINPUT(wVk=hexKeyCode))\n",
    "        user32.SendInput(1, ctypes.byref(x), ctypes.sizeof(x))\n",
    "\n",
    "    def release_key(self, hexKeyCode):\n",
    "        print(f\"Releasing key: {hexKeyCode}\")\n",
    "        x = INPUT(type=INPUT_KEYBOARD,\n",
    "                  ki=KEYBDINPUT(wVk=hexKeyCode,\n",
    "                                dwFlags=KEYEVENTF_KEYUP))\n",
    "        user32.SendInput(1, ctypes.byref(x), ctypes.sizeof(x))\n",
    "\n",
    "    def focus_window(self, hwnd):\n",
    "        ctypes.windll.user32.SetForegroundWindow(hwnd)\n",
    "        self.activate_remap()\n",
    "\n",
    "    def activate_remap(self):\n",
    "        time.sleep(0.5)\n",
    "        self.press_key(VK_LCONTROL)\n",
    "        self.press_key(VK_LMENU)\n",
    "        time.sleep(0.01)\n",
    "        self.release_key(VK_LCONTROL)\n",
    "        self.release_key(VK_LMENU)\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "    def hold_delay(self):\n",
    "        time.sleep(random.uniform(0.3, 0.35))\n",
    "\n",
    "    def process_keys(self, keys, action_type):\n",
    "        \"\"\"Recursively process key presses and releases.\"\"\"\n",
    "        if isinstance(keys, tuple):\n",
    "            for key in keys:\n",
    "                self.process_keys(key, action_type)\n",
    "        else:\n",
    "            if keys != 0:\n",
    "                if action_type == 'press':\n",
    "                    self.press_key(keys)\n",
    "                elif action_type == 'release':\n",
    "                    self.release_key(keys)\n",
    "\n",
    "    def execute_action(self, actionIndex):\n",
    "        action = valid_actions[actionIndex]\n",
    "        print(f\"Executing action: {action}\")  # Debug print\n",
    "        self.press_key(88)\n",
    "        if(actionIndex==0): #(82, 89, 68, 68, 89, 68, 84)\n",
    "            self.press_key(88)\n",
    "            self.press_key(82)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(82)\n",
    "            self.press_key(89)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(89)\n",
    "            self.press_key(68)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(68)\n",
    "            self.press_key(68)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.press_key(89)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(89)\n",
    "            self.press_key(68)\n",
    "            time.sleep(random.uniform(0.03, 0.05))\n",
    "            self.release_key(68)\n",
    "            self.press_key(68)\n",
    "            self.press_key(84)\n",
    "            time.sleep(random.uniform(0.3, 0.35))\n",
    "            self.release_key(84)\n",
    "        # Check if action is a tuple or list\n",
    "        elif isinstance(action, (tuple, list)):\n",
    "            self.press_key(88)\n",
    "            for key_sequence in action:\n",
    "                # Press keys in the current sequence\n",
    "                self.process_keys(key_sequence, 'press')\n",
    "                # Hold delay after pressing keys\n",
    "                self.hold_delay()\n",
    "                # Release keys in the current sequence\n",
    "                self.process_keys(key_sequence, 'release')\n",
    "        else:\n",
    "            # Handle single key press action\n",
    "            if action != 0:\n",
    "                self.process_keys(action, 'press')\n",
    "                self.hold_delay()\n",
    "                self.process_keys(action, 'release')\n",
    "            else:\n",
    "                print(f\"Invalid action structure: {action}\")\n",
    "        self.release_key(88)\n",
    "\n",
    "    def execute_actions(self, actions):\n",
    "        print(\"in execute_Actions\", actions)\n",
    "        for action in actions:\n",
    "            self.execute_action(action)\n",
    "            # time.sleep(random.uniform(0.01, 0.03))\n",
    "\n",
    "# Improved SumTree implementation with more efficient operations\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "        self.n_entries = 0\n",
    "        \n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        \n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "            \n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        \n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "            \n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "        \n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "    \n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        \n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        \n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "            \n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "            \n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        \n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "        \n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        \n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        \"\"\"More efficient batch sampling\"\"\"\n",
    "        indices = []\n",
    "        priorities = []\n",
    "        data = []\n",
    "        segment = self.total() / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            \n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, item = self.get(s)\n",
    "            \n",
    "            indices.append(idx)\n",
    "            priorities.append(p)\n",
    "            data.append(item)\n",
    "            \n",
    "        return indices, priorities, data\n",
    "\n",
    "# IMPROVED HYPERPARAMETERS\n",
    "IMAGE_STACK = 4 \n",
    "IMAGE_WIDTH = 84\n",
    "IMAGE_HEIGHT = 84\n",
    "HUBER_LOSS = 1.0  # Reduced for more stable learning\n",
    "LEARNING_RATE = 0.0001  # Lower learning rate for better stability\n",
    "MEMORY_CAPACITY = 500000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "MAX_EPSILON = 1.0\n",
    "MIN_EPSILON = 0.05\n",
    "EXPLORATION_STOP = 500000  # Shorter exploration for faster convergence\n",
    "LAMBDA = -math.log(0.01) / EXPLORATION_STOP\n",
    "UPDATE_TARGET_FREQUENCY = 2000  # More frequent updates\n",
    "FRAME_SKIP = 2  # Process every nth frame for efficiency\n",
    "REPLAY_PERIOD = 4  # How often to perform replay\n",
    "\n",
    "# Tracking variables\n",
    "best_eval_reward = float('-inf')\n",
    "running_reward = 0\n",
    "\n",
    "def huber_loss(y_true, y_pred):\n",
    "    \"\"\"Huber loss function for robust training\"\"\"\n",
    "    error = y_true - y_pred\n",
    "    cond = tf.abs(error) < HUBER_LOSS\n",
    "    \n",
    "    squared_loss = 0.5 * tf.square(error)\n",
    "    linear_loss = HUBER_LOSS * (tf.abs(error) - 0.5 * HUBER_LOSS)\n",
    "    \n",
    "    loss = tf.where(cond, squared_loss, linear_loss)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"More efficient frame preprocessing with proper normalization\"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(frame.shape) > 2 and frame.shape[2] > 1:\n",
    "        gray = np.dot(frame[...,:3], [0.299, 0.587, 0.114])\n",
    "    else:\n",
    "        gray = frame\n",
    "    \n",
    "    # Resize efficiently\n",
    "    resized = cv2.resize(gray, (IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Normalize to [0,1] range\n",
    "    normalized = resized / 255.0\n",
    "    \n",
    "    return normalized.astype(np.float32)\n",
    "\n",
    "def create_state(frame):\n",
    "    \"\"\"Create a state by stacking the same frame multiple times\"\"\"\n",
    "    # Create an array of stacked identical frames, more efficient than stack\n",
    "    frames = [frame] * IMAGE_STACK\n",
    "    return np.array(frames).transpose(1, 2, 0)  # HWC format\n",
    "\n",
    "def update_state(state, new_frame):\n",
    "    \"\"\"Update state with a new frame - efficient implementation\"\"\"\n",
    "    # Update state by shifting frames and adding new frame\n",
    "    # This avoids creating a new array each time\n",
    "    state = np.roll(state, -1, axis=2)  # Roll along channel dimension\n",
    "    state[:, :, -1] = new_frame  # Add new frame\n",
    "    return state\n",
    "\n",
    "def augment_state(state):\n",
    "    \"\"\"Data augmentation for better generalization\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    augmented = np.copy(state)\n",
    "    \n",
    "    # Random brightness adjustment\n",
    "    if random.random() < 0.3:\n",
    "        brightness_factor = random.uniform(0.8, 1.2)\n",
    "        augmented = np.clip(augmented * brightness_factor, 0, 1.0)\n",
    "    \n",
    "    # Random contrast adjustment\n",
    "    if random.random() < 0.3:\n",
    "        contrast_factor = random.uniform(0.8, 1.2)\n",
    "        mean = np.mean(augmented)\n",
    "        augmented = np.clip((augmented - mean) * contrast_factor + mean, 0, 1.0)\n",
    "        \n",
    "    # Adding random noise - improves robustness\n",
    "    if random.random() < 0.2:\n",
    "        noise = np.random.normal(0, 0.01, augmented.shape)\n",
    "        augmented = np.clip(augmented + noise, 0, 1.0)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Residual block for better gradient flow\"\"\"\n",
    "    def __init__(self, filters, kernel_size=3, strides=1, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        \n",
    "        self.conv1 = Conv2D(filters, kernel_size, strides=strides, padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(filters, kernel_size, padding='same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        \n",
    "        # Skip connection\n",
    "        self.skip = Conv2D(filters, 1, strides=strides, padding='same') if strides > 1 else None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        # Apply skip connection\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inputs)\n",
    "        else:\n",
    "            skip = inputs\n",
    "            \n",
    "        return tf.nn.relu(x + skip)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ResidualBlock, self).get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, input_shape, actionCnt, model=None, target_model=None, use_dueling=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.actionCnt = actionCnt\n",
    "        self.steps = 0\n",
    "        self.use_dueling = use_dueling\n",
    "        \n",
    "        # Setup TensorBoard with better logging\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.log_dir = f\"logs/fit/{current_time}\"\n",
    "        self.tensorboard_callback = TensorBoard(\n",
    "            log_dir=self.log_dir,\n",
    "            histogram_freq=1,\n",
    "            update_freq='epoch',\n",
    "            profile_batch=0\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.initial_learning_rate = LEARNING_RATE\n",
    "        \n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.target_model = target_model\n",
    "        else:\n",
    "            if use_dueling:\n",
    "                self.model = self._createDuelingModel()\n",
    "                self.target_model = self._createDuelingModel()\n",
    "                # Ensure target model has same weights\n",
    "                self.update_target_model()\n",
    "            else:\n",
    "                self.model = self._createModel()\n",
    "                self.target_model = self._createModel()\n",
    "                self.update_target_model()\n",
    "                \n",
    "    def _createModel(self):\n",
    "        \"\"\"Basic DQN model with residual connections\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input shape in HWC format - height, width, channels(frames)\n",
    "        input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_STACK)\n",
    "        \n",
    "        # First convolutional layer\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', \n",
    "                         input_shape=input_shape, padding='same'))\n",
    "        \n",
    "        # Add residual blocks for better gradient flow\n",
    "        model.add(ResidualBlock(64, 4, strides=2))\n",
    "        model.add(ResidualBlock(64, 3))\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=self.actionCnt, activation='linear'))\n",
    "        \n",
    "        # Use Adam optimizer with gradient clipping\n",
    "        opt = Adam(learning_rate=LEARNING_RATE, clipnorm=1.0)\n",
    "        model.compile(loss=huber_loss, optimizer=opt)\n",
    "        model.summary()\n",
    "        return model\n",
    "        \n",
    "    def _createDuelingModel(self):\n",
    "        \"\"\"Dueling DQN with residual connections\"\"\"\n",
    "        # Input in HWC format\n",
    "        input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_STACK)\n",
    "        input_layer = Input(shape=input_shape)\n",
    "        \n",
    "        # Convolutional layers with residual connections\n",
    "        conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='same')(input_layer)\n",
    "        res1 = ResidualBlock(64, 4, strides=2)(conv1)\n",
    "        res2 = ResidualBlock(64, 3)(res1)\n",
    "        \n",
    "        flat = Flatten()(res2)\n",
    "        \n",
    "        # Value stream (estimates state value)\n",
    "        value_fc = Dense(512, activation='relu')(flat)\n",
    "        value_dropout = Dropout(0.2)(value_fc)\n",
    "        value = Dense(1)(value_dropout)\n",
    "        \n",
    "        # Advantage stream (estimates action advantages)\n",
    "        adv_fc = Dense(512, activation='relu')(flat)\n",
    "        adv_dropout = Dropout(0.2)(adv_fc)\n",
    "        advantage = Dense(self.actionCnt)(adv_dropout)\n",
    "        \n",
    "        # Combine value and advantage (dueling architecture)\n",
    "        # Subtract mean to ensure identifiability\n",
    "        outputs = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
    "        \n",
    "        model = tf.keras.Model(inputs=input_layer, outputs=outputs)\n",
    "        opt = Adam(learning_rate=LEARNING_RATE, clipnorm=1.0)\n",
    "        model.compile(loss=huber_loss, optimizer=opt)\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def train(self, x, y, sample_weight=None, epochs=1, verbose=0):\n",
    "        \"\"\"Training with learning rate decay and callbacks\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Apply learning rate decay schedule\n",
    "        if self.steps > 50000:\n",
    "            lr = self.initial_learning_rate * 0.5\n",
    "        elif self.steps > 100000:\n",
    "            lr = self.initial_learning_rate * 0.25\n",
    "        elif self.steps > 200000:\n",
    "            lr = self.initial_learning_rate * 0.1\n",
    "        else:\n",
    "            lr = self.initial_learning_rate\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "        # Use larger batch size as training progresses for efficiency\n",
    "        batch_size = min(128, 32 + self.steps // 50000)\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            return self.model.fit(\n",
    "                x, y, \n",
    "                batch_size=batch_size, \n",
    "                sample_weight=sample_weight,\n",
    "                epochs=epochs, \n",
    "                verbose=verbose, \n",
    "                callbacks=[self.tensorboard_callback]\n",
    "            )\n",
    "        else:\n",
    "            return self.model.fit(\n",
    "                x, y, \n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs, \n",
    "                verbose=verbose, \n",
    "                callbacks=[self.tensorboard_callback]\n",
    "            )\n",
    "            \n",
    "    def train_on_batch(self, x, y, sample_weight=None):\n",
    "        \"\"\"Efficient batch training with gradient clipping\"\"\"\n",
    "        return self.model.train_on_batch(x, y, sample_weight=sample_weight)\n",
    "        \n",
    "    def predict(self, s, target=False):\n",
    "        \"\"\"Prediction with error handling\"\"\"\n",
    "        try:\n",
    "            if target:\n",
    "                return self.target_model.predict(s, verbose=0)\n",
    "            else:\n",
    "                return self.model.predict(s, verbose=0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction: {e}\")\n",
    "            print(f\"Input shape: {s.shape}\")\n",
    "            # Return zeros if prediction fails\n",
    "            if len(s.shape) == 4:  # Batch of states\n",
    "                return np.zeros((s.shape[0], self.actionCnt))\n",
    "            else:  # Single state\n",
    "                return np.zeros(self.actionCnt)\n",
    "                \n",
    "    def predict_one(self, s, target=False):\n",
    "        \"\"\"Predict Q-values for a single state\"\"\"\n",
    "        # Ensure correct shape: HWC format with batch dimension\n",
    "        if len(s.shape) == 3:  # If no batch dimension\n",
    "            s = np.expand_dims(s, axis=0)  # Add batch dimension\n",
    "        return self.predict(s, target).flatten()\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update target model with current weights\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def save_models(self, path, prefix=\"\"):\n",
    "        \"\"\"Save both models with error handling\"\"\"\n",
    "        try:\n",
    "            self.model.save(f\"{path}/{prefix}model.h5\")\n",
    "            self.target_model.save(f\"{path}/{prefix}target_model.h5\")\n",
    "            print(f\"Models saved to {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving models: {e}\")\n",
    "            return False\n",
    "\n",
    "# Enhanced Memory implementation with fixed-size efficient sampling\n",
    "class Memory:\n",
    "    def __init__(self, capacity, epsilon=0.01, alpha=0.6, beta=0.4, beta_increment=0.001):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.epsilon = epsilon  # small amount to avoid zero priority\n",
    "        self.alpha = alpha      # how much prioritization is used\n",
    "        self.beta = beta        # importance-sampling, increases to 1 over time\n",
    "        self.beta_increment = beta_increment\n",
    "        self.capacity = capacity\n",
    "        self.max_priority = 1.0\n",
    "        \n",
    "    def _getPriority(self, error):\n",
    "        \"\"\"Calculate priority based on TD error\"\"\"\n",
    "        return (np.abs(error) + self.epsilon) ** self.alpha\n",
    "        \n",
    "    def add(self, error, sample):\n",
    "        \"\"\"Add experience to memory with prioritization\"\"\"\n",
    "        # Use max priority for new samples to ensure they get sampled\n",
    "        p = self.max_priority if error is None else self._getPriority(error)\n",
    "        self.tree.add(p, sample)\n",
    "        \n",
    "        # Update max priority\n",
    "        if p > self.max_priority:\n",
    "            self.max_priority = p\n",
    "            \n",
    "    def sample(self, n):\n",
    "        \"\"\"Sample batch with importance sampling weights\"\"\"\n",
    "        # Increase beta over time for annealing\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        \n",
    "        # More efficient batch sampling\n",
    "        indices, priorities, samples = self.tree.get_batch(n)\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        sampling_probabilities = np.array(priorities) / self.tree.total()\n",
    "        weights = (self.capacity * sampling_probabilities) ** (-self.beta)\n",
    "        weights /= weights.max()  # Normalize for stability\n",
    "        \n",
    "        return list(zip(indices, samples)), weights\n",
    "        \n",
    "    def update(self, idx, error):\n",
    "        \"\"\"Update priorities based on new TD errors\"\"\"\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "        \n",
    "        # Update max priority\n",
    "        if p > self.max_priority:\n",
    "            self.max_priority = p\n",
    "            \n",
    "    def size(self):\n",
    "        \"\"\"Get current memory size\"\"\"\n",
    "        return self.tree.n_entries\n",
    "\n",
    "class LearningAgent:\n",
    "    steps = 0\n",
    "    latest_Q = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "    difficulty_level = 5  # Starting difficulty level (1-10)\n",
    "    combo_counter = 0\n",
    "    defensive_stance = False\n",
    "\n",
    "    def __init__(self, learning=False, epsilon=1.0, alpha=0.5):\n",
    "        self.input_handler = InputHandler()\n",
    "        self.learning = learning\n",
    "        # HWC format for input shape\n",
    "        self.inputShape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_STACK)\n",
    "        self.numActions = len(valid_actions)\n",
    "        self.model = Model(self.inputShape, self.numActions, use_dueling=True)\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "        # Efficient data tracking with fixed-size arrays\n",
    "        self.rewards_history = deque(maxlen=10000)\n",
    "        self.q_values_history = deque(maxlen=10000)\n",
    "        self.loss_history = deque(maxlen=10000)\n",
    "        \n",
    "        # Action tracking for diversity\n",
    "        self.prev_actions = deque(maxlen=10)\n",
    "        self.action_repeat_penalty = -0.1\n",
    "        \n",
    "        # Frame skipping counter\n",
    "        self.frame_skip_counter = 0\n",
    "        self.last_action = 0\n",
    "        \n",
    "        # Combo tracking\n",
    "        self.current_combo = 0\n",
    "        self.max_combo = 0\n",
    "        \n",
    "        # Add for performance monitoring\n",
    "        self.action_counts = Counter()\n",
    "        self.hits_landed = 0\n",
    "        self.damage_taken = 0\n",
    "        \n",
    "    def observe(self, sample):\n",
    "        \"\"\"Process and store experience in replay memory\"\"\"\n",
    "        s, a, r, s_ = sample\n",
    "        \n",
    "        # Apply data augmentation for better generalization\n",
    "        if random.random() < 0.2:\n",
    "            s = augment_state(s)\n",
    "            if s_ is not None:\n",
    "                s_ = augment_state(s_)\n",
    "        \n",
    "        # Get targets and errors for prioritized replay\n",
    "        x, y, errors = self.get_targets([(0, (s, a, r, s_))])\n",
    "        self.memory.add(errors[0], (s, a, r, s_))\n",
    "\n",
    "        # Update target network periodically\n",
    "        if self.steps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            self.model.update_target_model()\n",
    "            print(f\"Target network updated at step {self.steps}\")\n",
    "\n",
    "        # Update epsilon with decay schedule\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "        \n",
    "        # Track reward for metrics\n",
    "        self.rewards_history.append(r)\n",
    "        \n",
    "        # Log Q values periodically\n",
    "        if self.steps % 100 == 0:\n",
    "            self.q_values_history.append(np.mean(self.model.predict_one(s)))\n",
    "            \n",
    "            # Print progress\n",
    "            avg_reward = np.mean(list(self.rewards_history)[-100:]) if len(self.rewards_history) >= 100 else np.mean(list(self.rewards_history))\n",
    "            print(f\"Step: {self.steps}, Epsilon: {self.epsilon:.4f}, Avg Reward: {avg_reward:.4f}, Latest Q: {self.latest_Q:.4f}\")\n",
    "\n",
    "    def get_targets(self, batch):\n",
    "        \"\"\"Calculate target Q values using Double DQN\"\"\"\n",
    "        no_state = np.zeros(self.inputShape)\n",
    "\n",
    "        states = np.array([o[1][0] for o in batch])\n",
    "        states_ = np.array([(no_state if o[1][3] is None else o[1][3]) for o in batch])\n",
    "\n",
    "        # Predict Q values from current and target networks\n",
    "        p = self.model.predict(states)\n",
    "        p_ = self.model.predict(states_, target=False)  # model predictions for next states\n",
    "        pTarget_ = self.model.predict(states_, target=True)  # target model predictions for next states\n",
    "\n",
    "        x = np.zeros((len(batch), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_STACK))\n",
    "        y = np.zeros((len(batch), self.numActions))\n",
    "        errors = np.zeros(len(batch))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            o = batch[i][1]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i].copy()\n",
    "            oldVal = t[a]\n",
    "            \n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                # Double Deep Q-Learning update\n",
    "                a_ = np.argmax(p_[i])  # Action selection from online network\n",
    "                t[a] = r + GAMMA * pTarget_[i][a_]  # Value from target network\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "            # TD error for prioritized replay\n",
    "            errors[i] = abs(oldVal - t[a])\n",
    "            self.latest_Q = t[a]\n",
    "\n",
    "        return (x, y, errors)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy with diversity bonus\"\"\"\n",
    "        self.state = state\n",
    "        \n",
    "        # Apply frame skipping - repeat last action for efficiency\n",
    "        if self.frame_skip_counter < FRAME_SKIP and self.frame_skip_counter > 0:\n",
    "            self.frame_skip_counter += 1\n",
    "            return self.last_action\n",
    "        else:\n",
    "            self.frame_skip_counter = 1\n",
    "        \n",
    "        # Apply action repeating penalty to encourage diversity\n",
    "        action_penalties = np.zeros(self.numActions)\n",
    "        for prev_action in self.prev_actions:\n",
    "            action_penalties[prev_action] += self.action_repeat_penalty\n",
    "        \n",
    "        action = None\n",
    "\n",
    "        if not self.learning:\n",
    "            action = random.randint(0, len(valid_actions)-1)\n",
    "        else:\n",
    "            # Exploration vs exploitation\n",
    "            if random.uniform(0, 1) < self.epsilon:\n",
    "                action = random.randint(0, len(valid_actions)-1)\n",
    "                print(\"Exploring with random action:\", action)\n",
    "            else:\n",
    "                # Get Q values and apply penalties for repeated actions\n",
    "                q_values = self.model.predict_one(state)\n",
    "                adjusted_q_values = q_values + action_penalties\n",
    "                action = np.argmax(adjusted_q_values)\n",
    "                print(f\"Exploiting with action {action}, Q-value: {q_values[action]:.4f}\")\n",
    "        \n",
    "        # Update previous actions list for action diversity\n",
    "        self.prev_actions.append(action)\n",
    "        self.last_action = action\n",
    "        \n",
    "        # Update action counter for monitoring\n",
    "        self.action_counts[action] += 1\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def execute_action(self, action):\n",
    "        \"\"\"Execute selected action in game environment\"\"\"\n",
    "        self.input_handler.execute_action(action)\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Experience replay with prioritized sampling\"\"\"\n",
    "        # Wait until we have enough samples\n",
    "        if self.memory.size() < BATCH_SIZE:\n",
    "            return None\n",
    "            \n",
    "        # Sample batch with importance sampling weights\n",
    "        batch, is_weights = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # Get targets and errors for network update\n",
    "        x, y, errors = self.get_targets(batch)\n",
    "        \n",
    "        # Update priorities in memory\n",
    "        for i, (idx, _) in enumerate(batch):\n",
    "            self.memory.update(idx, errors[i])\n",
    "        \n",
    "        # Train the model with importance sampling weights\n",
    "        loss = self.model.train_on_batch(x, y, sample_weight=is_weights)\n",
    "        \n",
    "        # Track loss for metrics\n",
    "        self.loss_history.append(loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def play(self, state):\n",
    "        \"\"\"Used during evaluation/deployment without exploration\"\"\"\n",
    "        self.state = state\n",
    "        # Always choose the best action during play\n",
    "        action = np.argmax(self.model.predict_one(state))\n",
    "        \n",
    "        # Track action for monitoring\n",
    "        self.action_counts[action] += 1\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def adapt_difficulty(self, mean_reward):\n",
    "        \"\"\"Adjust opponent AI difficulty based on agent performance\"\"\"\n",
    "        if mean_reward > 5.0:\n",
    "            # Increase difficulty when agent performs well\n",
    "            self.difficulty_level = min(10, self.difficulty_level + 1)\n",
    "            print(f\"Increasing difficulty to level {self.difficulty_level}\")\n",
    "        elif mean_reward < -5.0:\n",
    "            # Decrease difficulty when agent struggles\n",
    "            self.difficulty_level = max(1, self.difficulty_level - 1)\n",
    "            print(f\"Decreasing difficulty to level {self.difficulty_level}\")\n",
    "        \n",
    "        # Return current difficulty level for game setup\n",
    "        return self.difficulty_level\n",
    "        \n",
    "    def update_combo(self, hit_successful):\n",
    "        \"\"\"Track combo counter for reward shaping\"\"\"\n",
    "        if hit_successful:\n",
    "            self.current_combo += 1\n",
    "            self.max_combo = max(self.max_combo, self.current_combo)\n",
    "        else:\n",
    "            self.current_combo = 0\n",
    "            \n",
    "    def get_combo_bonus(self):\n",
    "        \"\"\"Calculate bonus reward based on current combo\"\"\"\n",
    "        return min(0.5, self.current_combo * 0.1)\n",
    "        \n",
    "    def reset_episode_stats(self):\n",
    "        \"\"\"Reset per-episode tracking statistics\"\"\"\n",
    "        self.hits_landed = 0\n",
    "        self.damage_taken = 0\n",
    "        self.current_combo = 0\n",
    "        # Don't reset max_combo as that's a running statistic\n",
    "\n",
    "class Vision:\n",
    "    screen = {'top': 110, 'left': 0, 'width': 1920, 'height': 970}\n",
    "    leftHPCapture = {'top': 110, 'left': 240, 'width': 620, 'height': 20}\n",
    "    rightHPCapture = {'top': 110, 'left': 1043, 'width': 620, 'height': 20}\n",
    "    \n",
    "    positive = 1    # AI hit the opponent\n",
    "    negative = -1   # AI took a hit\n",
    "    \n",
    "    def __init__(self, side):\n",
    "        self.side = side\n",
    "        with mss.mss() as sct:\n",
    "            self.prevLeftHP = self.numpy_img_to_gray(np.array(sct.grab(self.leftHPCapture)))\n",
    "            self.prevRightHP = self.numpy_img_to_gray(np.array(sct.grab(self.rightHPCapture)))\n",
    "        \n",
    "        # For combo tracking\n",
    "        self.combo_counter = 0\n",
    "        self.last_hit_time = 0\n",
    "        self.combo_timeout = 1.0  # seconds between hits to count as combo\n",
    "        \n",
    "        # For defensive move detection\n",
    "        self.defensive_stance = False\n",
    "        self.last_damage_taken = 0\n",
    "        \n",
    "        # For state normalization\n",
    "        self.frames_seen = 0\n",
    "        self.running_mean = 0\n",
    "        self.running_std = 0\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.frame_times = deque(maxlen=100)\n",
    "        self.last_frame_time = time.time()\n",
    "        \n",
    "        # Add for performance monitoring\n",
    "        self.hits_landed = 0\n",
    "        self.damage_taken = 0\n",
    "        self.total_frames = 0\n",
    "        self.performance_monitor = None\n",
    "\n",
    "    def numpy_img_to_gray(self, img):\n",
    "        \"\"\"Convert RGB image to grayscale efficiently\"\"\"\n",
    "        return np.dot(img[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "    def get_current_screen(self):\n",
    "        \"\"\"Capture and preprocess current screen\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with mss.mss() as sct:\n",
    "            sct_img = sct.grab(self.screen)\n",
    "            img = Image.frombytes('RGB', sct_img.size, sct_img.rgb)\n",
    "            img = img.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)\n",
    "            currScreen = np.array(img)\n",
    "            \n",
    "        # Preprocess the screen\n",
    "        processed = preprocess_frame(currScreen)\n",
    "        \n",
    "        # Update running statistics for normalization (optional)\n",
    "        self.frames_seen += 1\n",
    "        delta = processed.mean() - self.running_mean\n",
    "        self.running_mean += delta / self.frames_seen\n",
    "        delta2 = processed.mean() - self.running_mean\n",
    "        self.running_std += delta * delta2\n",
    "        \n",
    "        # Track frame processing time\n",
    "        frame_time = time.time() - start_time\n",
    "        self.frame_times.append(frame_time)\n",
    "        \n",
    "        # Print average FPS periodically\n",
    "        if self.frames_seen % 100 == 0:\n",
    "            avg_frame_time = np.mean(self.frame_times)\n",
    "            fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0\n",
    "            print(f\"Average FPS: {fps:.1f}\")\n",
    "            \n",
    "            # Update performance monitor with FPS if available\n",
    "            if self.performance_monitor is not None:\n",
    "                self.performance_monitor.update_fps(fps)\n",
    "            \n",
    "        return processed\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Calculate reward based on game state with improved reward shaping\"\"\"\n",
    "        with mss.mss() as sct:\n",
    "            reward = 0\n",
    "            hit_landed = False\n",
    "            took_damage = False\n",
    "\n",
    "            currLeftHP = np.array(sct.grab(self.leftHPCapture))\n",
    "            currRightHP = np.array(sct.grab(self.rightHPCapture))\n",
    "            \n",
    "            # Convert to gray\n",
    "            currLeftHP = self.numpy_img_to_gray(currLeftHP)\n",
    "            currRightHP = self.numpy_img_to_gray(currRightHP)\n",
    "            \n",
    "            # Get the difference in previous vs current\n",
    "            diffLeftHP = self.prevLeftHP - currLeftHP\n",
    "            diffRightHP = self.prevRightHP - currRightHP\n",
    "            \n",
    "            # Round negative values up to 0\n",
    "            diffLeftHP = diffLeftHP.clip(min=0)\n",
    "            diffRightHP = diffRightHP.clip(min=0)\n",
    "            \n",
    "            # Calculate damage amounts for more granular rewards\n",
    "            left_damage_amount = (diffLeftHP > 125).sum() / 100.0\n",
    "            right_damage_amount = (diffRightHP > 125).sum() / 100.0\n",
    "            \n",
    "            # Basic reward calculation based on side\n",
    "            if left_damage_amount > 0.1:  # Left character took damage\n",
    "                if self.side == 'left':\n",
    "                    reward -= 1 + left_damage_amount  # Penalize based on damage amount\n",
    "                    self.defensive_stance = True\n",
    "                    self.last_damage_taken = time.time()\n",
    "                    self.combo_counter = 0\n",
    "                    took_damage = True\n",
    "                    self.damage_taken += left_damage_amount  # Track damage taken\n",
    "                else:\n",
    "                    reward += 1 + left_damage_amount  # Reward based on damage amount\n",
    "                    self.update_combo()\n",
    "                    hit_landed = True\n",
    "                    self.hits_landed += 1  # Track successful hits\n",
    "                    \n",
    "            if right_damage_amount > 0.1:  # Right character took damage\n",
    "                if self.side == 'right':\n",
    "                    reward -= 1 + right_damage_amount\n",
    "                    self.defensive_stance = True\n",
    "                    self.last_damage_taken = time.time()\n",
    "                    self.combo_counter = 0\n",
    "                    took_damage = True\n",
    "                    self.damage_taken += right_damage_amount  # Track damage taken\n",
    "                else:\n",
    "                    reward += 1 + right_damage_amount\n",
    "                    self.update_combo()\n",
    "                    hit_landed = True\n",
    "                    self.hits_landed += 1  # Track successful hits\n",
    "            \n",
    "            # Add combo bonus\n",
    "            if self.combo_counter > 1:\n",
    "                combo_bonus = min(self.combo_counter * 0.2, 1.5)\n",
    "                reward += combo_bonus\n",
    "                \n",
    "            # Add time penalty (small) to encourage action\n",
    "            time_penalty = -0.005\n",
    "            reward += time_penalty\n",
    "            \n",
    "            # Add defensive bonus if successful defense\n",
    "            if self.defensive_stance and time.time() - self.last_damage_taken > 2.0:\n",
    "                if not took_damage:\n",
    "                    reward += 0.2  # Bonus for successful defense\n",
    "                self.defensive_stance = False\n",
    "                \n",
    "            # Add position reward based on screen analysis (optional)\n",
    "            # This would require additional screen analysis\n",
    "                \n",
    "            # Set previous frame data to current frame data\n",
    "            self.prevLeftHP = currLeftHP\n",
    "            self.prevRightHP = currRightHP\n",
    "            \n",
    "            # Increment frame counter\n",
    "            self.total_frames += 1\n",
    "            \n",
    "            # Track FPS every 100 frames\n",
    "            if self.total_frames % 100 == 0:\n",
    "                avg_frame_time = np.mean(self.frame_times)\n",
    "                fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0\n",
    "                # Update the performance monitor with FPS if it exists\n",
    "                if hasattr(self, 'performance_monitor') and self.performance_monitor is not None:\n",
    "                    self.performance_monitor.update_fps(fps)\n",
    "\n",
    "        return reward, hit_landed, took_damage\n",
    "    \n",
    "    def update_combo(self):\n",
    "        \"\"\"Update combo counter based on timing between hits\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Check if this hit is part of a combo (within timeout)\n",
    "        if current_time - self.last_hit_time < self.combo_timeout:\n",
    "            self.combo_counter += 1\n",
    "            print(f\"Combo x{self.combo_counter}!\")\n",
    "        else:\n",
    "            # Start new combo\n",
    "            self.combo_counter = 1\n",
    "            \n",
    "        self.last_hit_time = current_time\n",
    "        return self.combo_counter\n",
    "\n",
    "TRIAL = 1\n",
    "TOTAL_TESTS = 1\n",
    "TOTAL_EPISODES = 100\n",
    "EVAL_INTERVAL = 5  # Evaluate every 5 episodes\n",
    "\n",
    "def save_models(agent, curr_test, is_best=False):\n",
    "    \"\"\"Save model with enhanced naming and best model tracking\"\"\"\n",
    "    directory = 'Models/trial 2'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    # Save regular checkpoint\n",
    "    model_name = f\"/TekkenBotDDQN_{curr_test}.h5\"\n",
    "    target_name = f\"/TekkenBotDDQN_Target_{curr_test}.h5\"\n",
    "    \n",
    "    if is_best:\n",
    "        # Save as best model\n",
    "        model_name = \"/TekkenBotDDQN_best.h5\"\n",
    "        target_name = \"/TekkenBotDDQN_Target_best.h5\"\n",
    "\n",
    "        # Save model architecture as JSON\n",
    "    model_json = agent.model.model.to_json()\n",
    "    with open(f\"{directory}{model_name.replace('.h5', '.json')}\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    # Save target model architecture as JSON\n",
    "    target_json = agent.model.target_model.to_json()\n",
    "    with open(f\"{directory}{target_name.replace('.h5', '.json')}\", \"w\") as json_file:\n",
    "        json_file.write(target_json)\n",
    "        \n",
    "    # Save both models\n",
    "    agent.model.model.save(directory + model_name, overwrite=True)\n",
    "    agent.model.target_model.save(directory + target_name, overwrite=True)\n",
    "    \n",
    "    print(f\"Models saved: {model_name} and {target_name}\")\n",
    "    \n",
    "    # Save training history\n",
    "    if len(agent.rewards_history) > 0:\n",
    "        np.save(f\"{directory}/rewards_history.npy\", np.array(list(agent.rewards_history)))\n",
    "    if len(agent.q_values_history) > 0:\n",
    "        np.save(f\"{directory}/q_values_history.npy\", np.array(list(agent.q_values_history)))\n",
    "    if len(agent.loss_history) > 0:\n",
    "        np.save(f\"{directory}/loss_history.npy\", np.array(list(agent.loss_history)))\n",
    "        \n",
    "    # Save replay memory samples for later resume\n",
    "    if agent.memory.size() > 1000:\n",
    "        print(\"Saving memory samples...\")\n",
    "        mem_indices, mem_priorities, mem_samples = [], [], []\n",
    "        for _ in range(1000):  # Save a subset of memory\n",
    "            idx, p, sample = agent.memory.tree.get(random.uniform(0, agent.memory.tree.total()))\n",
    "            mem_indices.append(idx)\n",
    "            mem_priorities.append(p)\n",
    "            mem_samples.append(sample)\n",
    "        \n",
    "        np.save(f\"{directory}/memory_samples.npy\", np.array(mem_samples, dtype=object))\n",
    "        print(\"Memory samples saved.\")\n",
    "\n",
    "def evaluate(agent, num_episodes=10, performance_monitor=None):\n",
    "    \"\"\"More comprehensive evaluation function with performance monitoring\"\"\"\n",
    "    global best_eval_reward\n",
    "    \n",
    "    agent.input_handler.activate_remap()\n",
    "    vision = Vision('left')\n",
    "    \n",
    "    # Set performance monitor for vision if provided\n",
    "    if performance_monitor is not None:\n",
    "        vision.performance_monitor = performance_monitor\n",
    "    \n",
    "    rewards = []\n",
    "    episode_lengths = []\n",
    "    q_values = []\n",
    "    \n",
    "    print(f\"Starting evaluation for {num_episodes} episodes...\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Reset episode stats\n",
    "        agent.reset_episode_stats()\n",
    "        vision.hits_landed = 0\n",
    "        vision.damage_taken = 0\n",
    "        \n",
    "        # Get initial observation and create initial state\n",
    "        init_frame = vision.get_current_screen()\n",
    "        \n",
    "        # Create state with stacked frames (more efficient with array)\n",
    "        state = np.array([init_frame] * IMAGE_STACK).transpose(1, 2, 0)\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while not done:\n",
    "            # Use deterministic policy (no exploration)\n",
    "            actionIndex = agent.play(state)\n",
    "            agent.execute_action(actionIndex)\n",
    "            \n",
    "            # Capture new observation and reward\n",
    "            new_obs = vision.get_current_screen()\n",
    "            reward, hit, damage = vision.get_reward()\n",
    "            \n",
    "            # Update agent's internal combo counter\n",
    "            if hit:\n",
    "                agent.update_combo(True)\n",
    "                agent.hits_landed += 1\n",
    "            elif damage:\n",
    "                agent.update_combo(False)\n",
    "                agent.damage_taken += 1\n",
    "            \n",
    "            # Track Q-values during evaluation\n",
    "            q_vals = agent.model.predict_one(state)\n",
    "            q_values.append(np.mean(q_vals))\n",
    "            \n",
    "            # Efficient state update\n",
    "            state = update_state(state, new_obs)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # End episode after 60 seconds\n",
    "            if time.time() - start_time > 59:\n",
    "                done = True\n",
    "                \n",
    "        rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        print(f\"Eval Episode {episode+1}: Reward = {total_reward:.2f}, Steps = {steps}, Max Combo: {agent.max_combo}\")\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    mean_length = np.mean(episode_lengths)\n",
    "    std_length = np.std(episode_lengths)\n",
    "    mean_q = np.mean(q_values) if q_values else 0\n",
    "    \n",
    "    print(f'Evaluation Results:')\n",
    "    print(f'Average Reward: {mean_reward:.2f} ± {std_reward:.2f}')\n",
    "    print(f'Average Episode Length: {mean_length:.2f} ± {std_length:.2f}')\n",
    "    print(f'Average Q Value: {mean_q:.4f}')\n",
    "    \n",
    "    # Save best model if this is the best performance\n",
    "    if mean_reward > best_eval_reward:\n",
    "        best_eval_reward = mean_reward\n",
    "        save_models(agent, -1, is_best=True)\n",
    "        print(f\"New best model saved with reward: {mean_reward:.2f}\")\n",
    "    \n",
    "    # Update performance monitor if provided\n",
    "    if performance_monitor is not None:\n",
    "        eval_results = {\n",
    "            'rewards': rewards,\n",
    "            'mean_reward': mean_reward,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'mean_length': mean_length,\n",
    "            'mean_q': mean_q,\n",
    "            'max_combo': agent.max_combo\n",
    "        }\n",
    "        performance_monitor.update_eval(agent.steps, eval_results)\n",
    "    \n",
    "    # Return metrics for tracking\n",
    "    return {\n",
    "        'rewards': rewards,\n",
    "        'mean_reward': mean_reward,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'mean_length': mean_length,\n",
    "        'mean_q': mean_q,\n",
    "        'max_combo': agent.max_combo\n",
    "    }\n",
    "\n",
    "def import_model(agent):\n",
    "    \"\"\"Import saved model with better error handling\"\"\"\n",
    "    try:\n",
    "        # Define custom objects including your ResidualBlock and loss function\n",
    "        custom_objects = {\n",
    "            'ResidualBlock': ResidualBlock,\n",
    "            'huber_loss': huber_loss\n",
    "        }\n",
    "        \n",
    "        # Try to load best model first\n",
    "        best_model_path = 'Models/trial 2/TekkenBotDDQN_best.h5'\n",
    "        best_target_path = 'Models/trial 2/TekkenBotDDQN_Target_best.h5'\n",
    "        \n",
    "        if os.path.exists(best_model_path) and os.path.exists(best_target_path):\n",
    "            agent.model = Model(agent.inputShape, agent.numActions,\n",
    "                load_model(best_model_path, custom_objects=custom_objects),\n",
    "                load_model(best_target_path, custom_objects=custom_objects),\n",
    "                use_dueling=True)\n",
    "            print('Best model loaded successfully.')\n",
    "            \n",
    "            # Try to load saved memory samples\n",
    "            memory_path = 'Models/trial 2/memory_samples.npy'\n",
    "            if os.path.exists(memory_path):\n",
    "                try:\n",
    "                    memory_samples = np.load(memory_path, allow_pickle=True)\n",
    "                    print(f\"Loading {len(memory_samples)} memory samples...\")\n",
    "                    for sample in memory_samples:\n",
    "                        agent.memory.add(None, sample)  # Add with max priority\n",
    "                    print(f\"Memory initialized with {agent.memory.size()} samples.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading memory samples: {e}\")\n",
    "                    \n",
    "            return True\n",
    "            \n",
    "        # Fall back to regular model\n",
    "        model_path = 'Models/trial 2/TekkenBotDDQN_1.h5'\n",
    "        target_path = 'Models/trial 2/TekkenBotDDQN_Target_1.h5'\n",
    "        \n",
    "        if os.path.exists(model_path) and os.path.exists(target_path):\n",
    "            agent.model = Model(agent.inputShape, agent.numActions,\n",
    "                load_model(model_path, custom_objects=custom_objects),\n",
    "                load_model(target_path, custom_objects=custom_objects),\n",
    "                use_dueling=True)\n",
    "            print('Model loaded successfully.')\n",
    "            return True\n",
    "            \n",
    "        print('No existing model found. Starting with fresh model.')\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error loading model:')\n",
    "        print(e)\n",
    "        print('Starting with fresh model.')\n",
    "        return False\n",
    "    \n",
    "def play(agent):\n",
    "    \"\"\"Play mode for the trained agent with performance monitoring\"\"\"\n",
    "    # Create performance monitor for play mode\n",
    "    performance_monitor = PerformanceMonitor(save_dir='performance_data/play')\n",
    "    \n",
    "    agent.input_handler.activate_remap()\n",
    "    vision = Vision('left')\n",
    "    vision.performance_monitor = performance_monitor\n",
    "    \n",
    "    # Initialize with first observation\n",
    "    initial_screen = vision.get_current_screen()\n",
    "    # More efficient state creation with array\n",
    "    state = np.array([initial_screen] * IMAGE_STACK).transpose(1, 2, 0)\n",
    "    \n",
    "    print(\"Starting play mode with trained agent. Press Ctrl+C to stop.\")\n",
    "    \n",
    "    # Reset stats for play session\n",
    "    agent.reset_episode_stats()\n",
    "    episode_start_time = time.time()\n",
    "    episode_rewards = 0\n",
    "    episode_actions = []\n",
    "    episode_num = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Choose action using greedy policy\n",
    "            actionIndex = agent.play(state)\n",
    "            agent.execute_action(actionIndex)\n",
    "            \n",
    "            # Track actions\n",
    "            episode_actions.append(actionIndex)\n",
    "            \n",
    "            # Get new observation\n",
    "            screenCap = vision.get_current_screen()\n",
    "            reward, hit, damage = vision.get_reward()\n",
    "            \n",
    "            # More efficient state update\n",
    "            state = update_state(state, screenCap)\n",
    "            \n",
    "            # Update combo counter\n",
    "            if hit:\n",
    "                agent.update_combo(True)\n",
    "                agent.hits_landed += 1\n",
    "                print(f\"Hit landed! Combo: {agent.current_combo}\")\n",
    "            elif damage:\n",
    "                agent.update_combo(False)\n",
    "                agent.damage_taken += 1\n",
    "                print(\"Took damage!\")\n",
    "                \n",
    "            # Accumulate reward\n",
    "            episode_rewards += reward\n",
    "            \n",
    "            # End of episode (60 seconds) - update performance monitor\n",
    "            if time.time() - episode_start_time > 59:\n",
    "                # Get average Q-value\n",
    "                avg_q = np.mean(agent.model.predict_one(state))\n",
    "                \n",
    "                # Update performance monitor\n",
    "                performance_monitor.update(\n",
    "                    episode=episode_num,\n",
    "                    reward=episode_rewards,\n",
    "                    steps=agent.steps,\n",
    "                    avg_q=avg_q,\n",
    "                    epsilon=0.0,  # No exploration in play mode\n",
    "                    loss=None,\n",
    "                    max_combo=agent.max_combo,\n",
    "                    difficulty_level=agent.difficulty_level,\n",
    "                    hits=agent.hits_landed,\n",
    "                    damage_taken=agent.damage_taken,\n",
    "                    actions=episode_actions\n",
    "                )\n",
    "                \n",
    "                \n",
    "                print(f\"Play Episode {episode_num+1} completed. Reward: {episode_rewards:.2f}, Max Combo: {agent.max_combo}\")\n",
    "                \n",
    "                # Reset for next episode\n",
    "                episode_num += 1\n",
    "                episode_start_time = time.time()\n",
    "                episode_rewards = 0\n",
    "                episode_actions = []\n",
    "                agent.reset_episode_stats()\n",
    "            \n",
    "            # Optional: add short sleep to control action rate\n",
    "            time.sleep(0.05)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Play mode stopped.')\n",
    "        # Save final performance data\n",
    "        performance_monitor.print_summary()\n",
    "        performance_monitor.plot_and_save()\n",
    "\n",
    "def run(agent):\n",
    "    \"\"\"Enhanced training function with improved monitoring and visualization\"\"\"\n",
    "    # Create performance monitor\n",
    "    performance_monitor = PerformanceMonitor(save_dir='performance_data')\n",
    "    \n",
    "    agent.input_handler.activate_remap()\n",
    "    vision = Vision('left')\n",
    "    \n",
    "    # Set performance monitor for vision\n",
    "    vision.performance_monitor = performance_monitor\n",
    "    \n",
    "    # Get first observation and create initial state\n",
    "    first_obs = vision.get_current_screen()\n",
    "    # More efficient state creation with array instead of stack\n",
    "    state = np.array([first_obs] * IMAGE_STACK).transpose(1, 2, 0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    episode_start_time = time.time()\n",
    "    global_start_time = time.time()\n",
    "    \n",
    "    # Training statistics\n",
    "    reward_total = 0\n",
    "    episode = 0\n",
    "    true_episode = 0\n",
    "    curr_test = 1\n",
    "    record_size = TOTAL_EPISODES * TOTAL_TESTS + 1\n",
    "    # Tracking more metrics\n",
    "    reward_per_episode = np.zeros((record_size, 6))  # Added column for max combo\n",
    "    i = 0\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    eval_results = []\n",
    "    \n",
    "    # Reset episode stats\n",
    "    agent.reset_episode_stats()\n",
    "    vision.hits_landed = 0\n",
    "    vision.damage_taken = 0\n",
    "    \n",
    "    # For action distribution tracking\n",
    "    episode_actions = []\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting training. Press Ctrl+C to stop.\")\n",
    "        while True:\n",
    "            # Choose action based on current policy\n",
    "            actionIndex = agent.choose_action(state)\n",
    "            agent.execute_action(actionIndex)\n",
    "            \n",
    "            # Track actions for distribution analysis\n",
    "            episode_actions.append(actionIndex)\n",
    "            \n",
    "            # Get new observation and reward\n",
    "            screenCap = vision.get_current_screen()\n",
    "            reward, hit, damage = vision.get_reward()\n",
    "            \n",
    "            # Update agent's combo counter\n",
    "            if hit:\n",
    "                agent.update_combo(True)\n",
    "                agent.hits_landed += 1\n",
    "            elif damage:\n",
    "                agent.update_combo(False)\n",
    "                agent.damage_taken += 1\n",
    "            \n",
    "            # Efficient state update\n",
    "            next_state = update_state(state.copy(), screenCap)\n",
    "            \n",
    "            # Store experience in replay memory\n",
    "            agent.observe((state, actionIndex, reward, next_state))\n",
    "            \n",
    "            # Perform experience replay periodically\n",
    "            if agent.steps % REPLAY_PERIOD == 0:\n",
    "                loss = agent.replay()\n",
    "                if loss is not None:\n",
    "                    agent.loss_history.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            reward_total += reward\n",
    "            \n",
    "            # End of episode (60 seconds)\n",
    "            if time.time() - episode_start_time > 59:\n",
    "                # Record episode stats\n",
    "                avg_q = np.mean(list(agent.q_values_history)[-100:]) if agent.q_values_history else 0\n",
    "                avg_loss = np.mean(list(agent.loss_history)[-100:]) if agent.loss_history and len(agent.loss_history) >= 100 else None\n",
    "                \n",
    "                reward_per_episode[i] = [true_episode, reward_total, agent.steps, avg_q, agent.difficulty_level, agent.max_combo]\n",
    "                \n",
    "                # Update performance monitor\n",
    "                performance_monitor.update(\n",
    "                    episode=true_episode,\n",
    "                    reward=reward_total,\n",
    "                    steps=agent.steps,\n",
    "                    avg_q=avg_q,\n",
    "                    epsilon=agent.epsilon,\n",
    "                    loss=avg_loss,\n",
    "                    max_combo=agent.max_combo,\n",
    "                    difficulty_level=agent.difficulty_level,\n",
    "                    hits=agent.hits_landed,\n",
    "                    damage_taken=agent.damage_taken,\n",
    "                    actions=episode_actions\n",
    "                )\n",
    "                \n",
    "                # Generate plots every 5 episodes\n",
    "                \n",
    "                # Calculate training speed\n",
    "                elapsed = time.time() - global_start_time\n",
    "                steps_per_second = agent.steps / elapsed if elapsed > 0 else 0\n",
    "                \n",
    "                print(f\"Episode {true_episode+1} completed. Reward: {reward_total:.2f}, Steps: {agent.steps}, \"\n",
    "                      f\"Epsilon: {agent.epsilon:.4f}, Max Combo: {agent.max_combo}, \"\n",
    "                      f\"Steps/sec: {steps_per_second:.1f}\")\n",
    "                \n",
    "                episode += 1\n",
    "                true_episode += 1\n",
    "                i += 1\n",
    "                episode_start_time = time.time()\n",
    "                \n",
    "                # Reset episode stats\n",
    "                agent.reset_episode_stats()\n",
    "                vision.hits_landed = 0\n",
    "                vision.damage_taken = 0\n",
    "                episode_actions = []\n",
    "                \n",
    "                # Evaluate periodically\n",
    "                if episode % EVAL_INTERVAL == 0:\n",
    "                    print(f\"Performing evaluation after episode {true_episode}\")\n",
    "                    eval_result = evaluate(agent, num_episodes=5, performance_monitor=performance_monitor)\n",
    "                    eval_results.append(eval_result)\n",
    "                    \n",
    "                    # Adjust difficulty based on performance\n",
    "                    agent.adapt_difficulty(eval_result['mean_reward'])\n",
    "                \n",
    "                # Reset for next episode\n",
    "                reward_total = 0\n",
    "                \n",
    "            # End of test segment\n",
    "            if episode >= TOTAL_EPISODES:\n",
    "                if curr_test >= TOTAL_TESTS:\n",
    "                    save_models(agent, curr_test)\n",
    "                    print(\"Training completed. Final model saved.\")\n",
    "                    # Final performance plots\n",
    "                    performance_monitor.plot_and_save()\n",
    "                    break\n",
    "                else:\n",
    "                    save_models(agent, curr_test)\n",
    "                    curr_test += 1\n",
    "                    episode = 0\n",
    "                    print(f\"Starting test segment {curr_test}\")\n",
    "                    \n",
    "                    # Evaluate between test segments\n",
    "                    evaluate(agent, num_episodes=10, performance_monitor=performance_monitor)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user.\")\n",
    "    \n",
    "    finally:\n",
    "        # Save final models and training data\n",
    "        print(\"Saving models and training history...\")\n",
    "        save_models(agent, curr_test)\n",
    "        \n",
    "        performance_monitor.plot_and_save()\n",
    "        performance_monitor.print_summary()\n",
    "        # Save episode rewards and evaluation results\n",
    "        np.savetxt('Models/trial 2/episodesAndRewards.txt', reward_per_episode, fmt='%.4f')\n",
    "        print(\"Episodes and rewards saved to episodesAndRewards.txt\")\n",
    "        \n",
    "        # Save evaluation results\n",
    "        if eval_results:\n",
    "            eval_dir = 'Models/trial 2/evaluations'\n",
    "            if not os.path.exists(eval_dir):\n",
    "                os.makedirs(eval_dir)\n",
    "            np.save(f\"{eval_dir}/eval_results.npy\", eval_results)\n",
    "            \n",
    "        # Final performance plots\n",
    "        performance_monitor.plot_and_save()\n",
    "            \n",
    "        # Final statistics\n",
    "        elapsed = time.time() - global_start_time\n",
    "        hours = elapsed // 3600\n",
    "        minutes = (elapsed % 3600) // 60\n",
    "        seconds = elapsed % 60\n",
    "        \n",
    "        print(f\"Training complete. Total time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "        print(f\"Total steps: {agent.steps}, Steps per second: {agent.steps / elapsed:.1f}\")\n",
    "        print(f\"Max combo achieved: {agent.max_combo}\")\n",
    "        \n",
    "        # Print performance summary\n",
    "        performance_monitor.print_summary()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Create the learning agent with dueling network and improved hyperparameters\n",
    "        agent = LearningAgent(learning=True, epsilon=MAX_EPSILON, alpha=0.6)\n",
    "        \n",
    "        # Try to load existing model if available\n",
    "        imported = import_model(agent)\n",
    "        \n",
    "        # Ask user whether to train or play\n",
    "        mode = input(\"Enter mode (train/play): \").lower()\n",
    "        \n",
    "        if mode == 'train':\n",
    "            print(\"Starting training mode...\")\n",
    "            run(agent)\n",
    "        elif mode == 'play':\n",
    "            if not imported:\n",
    "                print(\"Warning: No model found for play mode. Training a simple model first...\")\n",
    "                # Do a short training session\n",
    "                run(agent)\n",
    "            print(\"Starting play mode...\")\n",
    "            play(agent)\n",
    "        else:\n",
    "            print(\"Invalid mode. Please enter 'train' or 'play'.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        # Print stack trace for debugging\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        print('Session ended. Thanks for playing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxerxxerxtxtxaxaxxeyxxeyxxxaxaxaxaxxdsxxdsxsxsxaxaxxeyxxeyxxawxxawxxxxawxxawxyxyxxyyyyxxyy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tekken_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
